{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPi8FnHLRcY6CyVb3sFPa+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asungii/quantization-experiments/blob/main/SmoothQuant_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Installations</h1>\n",
        "\n",
        "From SmoothQuant repo. Because the specified CUDA compiler is 11.3, and the default Colab CUDA version is as of 7/29/24 12.2, we have to change the CUDA version in Colab.\n",
        "\n",
        "I found info on how to do this from:\n",
        "\n",
        "https://medium.com/@ajithkumarv/how-to-modify-cuda-gcc-python-versions-in-colab-584ed4113157\n",
        "\n",
        "https://github.com/googlecolab/colabtools/issues/4214#issuecomment-1859155789\n",
        "\n",
        "https://vitalitylearning.medium.com/running-cuda-in-google-colab-525a92efcf75"
      ],
      "metadata": {
        "id": "nKLSnmrnyb2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6NI2WxlYpUM",
        "outputId": "ef0bbb3f-994a-4751-bb00-5090511e41c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.10/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu113 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.7.4)\n",
            "Requirement already satisfied: transformers==4.36.0 in /usr/local/lib/python3.10/dist-packages (4.36.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (4.66.4)\n",
            "Collecting numpy>=1.17 (from transformers==4.36.0)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.1\n",
            "    Uninstalling numpy-2.0.1:\n",
            "      Successfully uninstalled numpy-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.0 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "# setup is from the mit-han-lab GitHub\n",
        "\n",
        "# these bits are not necessary for Google Colab\n",
        "# !conda create -n smoothquant python=3.8\n",
        "# !conda activate smoothquant\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers==4.36.0 accelerate datasets zstandard\n",
        "\n",
        "# !python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "45Rh36UA8efT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "\n",
        "# well it's 12.2. that's a bit of an issue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azoig1mPz7bV",
        "outputId": "8787883e-943f-4d9d-836b-44d51ef26d1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
        "!sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.3\n",
        "\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrTVEc6byXAs",
        "outputId": "556a486f-d753-43fb-9a58-1ad5eb5ba7a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-29 15:53:20--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu2004.pin’\n",
            "\n",
            "\rcuda-ubuntu2004.pin   0%[                    ]       0  --.-KB/s               \rcuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-29 15:53:20 (6.23 MB/s) - ‘cuda-ubuntu2004.pin’ saved [190/190]\n",
            "\n",
            "--2024-07-29 15:53:20--  https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2286573544 (2.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb.1’\n",
            "\n",
            "o-ubuntu2004-11-3-l  51%[=========>          ]   1.11G   321MB/s    eta 4s     ^C\n",
            "(Reading database ... 133582 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) over (11.3.0-465.19.01-1) ...\n",
            "Setting up cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 1s (215 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-11.3-1' for regex 'cuda-11.3'\n",
            "Note, selecting 'cuda-11-3' for regex 'cuda-11.3'\n",
            "cuda-11-3 is already the newest version (11.3.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUDA_PATH=/usr/local/cuda-11.3/\n",
        "#!export PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/"
      ],
      "metadata": {
        "id": "PuthK8ce16jt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claude says that the !export doesn't persist outside of the cell. Whoops."
      ],
      "metadata": {
        "id": "qzBJtnmg9CcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek_OMQGz8aeR",
        "outputId": "e8f758c9-8813-492e-9ec0-b62d6e08bf3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.3'\n",
        "os.environ['PATH'] = '/usr/local/cuda-11.3/bin:' + os.environ['PATH']\n",
        "\n",
        "# THIS WORKED! YAY!!"
      ],
      "metadata": {
        "id": "cBSdi8Nt9FoS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we install a GCC version compatible:"
      ],
      "metadata": {
        "id": "PorxhJIg-1q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --remove-all gcc\n",
        "!sudo update-alternatives --remove-all g++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v3F4i02JK1g",
        "outputId": "cb04ad7d-4867-4ce1-a9c2-4ca1793c25cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: error: no alternatives for g++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential software-properties-common -y\n",
        "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc-9 g++-9 -y\n",
        "!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9\n",
        "!gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaBumXpw-1TR",
        "outputId": "afb3a781-cda4-4f6f-f2c5-651f7a4e7023"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r0% [1 InRelease 0 B]\r                    \rIgn:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r                    \r0% [Working]\r            \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [2 Release 0 B/564 B 0%]\r                           \r0% [Working]\r            \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [2 Release 0 B/564 B 0%] [Connecting to archive.ubuntu.com (91.189.91.83)] [\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connectin\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\r                                                                               \rIgn:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
            "PPA publishes dbgsym, you may need to include 'main/debug' component\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu/ jammy main'\n",
            "Description:\n",
            "Toolchain test builds; see https://wiki.ubuntu.com/ToolChain\n",
            "\n",
            "More info: https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test\n",
            "Adding repository.\n",
            "Found existing deb entry in /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding deb entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Found existing deb-src entry in /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/ubuntu-toolchain-r-ubuntu-test.gpg with fingerprint 60C317803A41BA51845E371A1E9377A2BA9EF27F\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:11 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++-9 is already the newest version (9.5.0-1ubuntu1~22.04).\n",
            "gcc-9 is already the newest version (9.5.0-1ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
            "update-alternatives: using /usr/bin/gcc-9 to provide /usr/bin/gcc (gcc) in auto mode\n",
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works, but you will have to deal with this girthy installation every time you want to use this notebook."
      ],
      "metadata": {
        "id": "VQUnsKUL9SMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "%cd /usr/local/\n",
        "%pwd\n",
        "%ls\n",
        "! echo $PATH\n",
        "! python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEQw31bw2efT",
        "outputId": "22294f8f-85dd-49b5-d09e-9ea681a37ead"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n",
            "/usr/local\n",
            "\u001b[0m\u001b[01;34mbin\u001b[0m/    \u001b[01;36mcuda-11\u001b[0m@    \u001b[01;34mcuda-12.2\u001b[0m/  \u001b[01;34m_gcs_config_ops.so\u001b[0m/  \u001b[01;34mlib64\u001b[0m/  \u001b[01;34msbin\u001b[0m/         \u001b[01;34msrc\u001b[0m/\n",
            "\u001b[01;34mcolab\u001b[0m/  \u001b[01;34mcuda-11.3\u001b[0m/  \u001b[01;34metc\u001b[0m/        \u001b[01;34minclude\u001b[0m/             \u001b[01;36mman\u001b[0m@    \u001b[01;34mshare\u001b[0m/\n",
            "\u001b[01;36mcuda\u001b[0m@   \u001b[01;36mcuda-12\u001b[0m@    \u001b[01;34mgames\u001b[0m/      \u001b[01;34mlib\u001b[0m/                 \u001b[01;34mopt\u001b[0m/    \u001b[01;34msmoothquant\u001b[0m/\n",
            "/usr/local/cuda-11.3/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9o_FSLH9ctO",
        "outputId": "4d639fd3-eb72-41b5-cc67-bb0d48fc2404"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/mit-han-lab/smoothquant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muFQ_9Zgfk2L",
        "outputId": "7eaaf39f-2098-48f2-9f9a-00a616b16fcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'smoothquant' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/smoothquant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jsoZyglf5x0",
        "outputId": "6cb1c719-14b0-44f7-81c2-a59cee2dc075"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/smoothquant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0teKFDFGf_lT",
        "outputId": "56b52b25-4069-47a9-be5c-d4f8b36ac670"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/smoothquant\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: smoothquant\n",
            "  Attempting uninstall: smoothquant\n",
            "    Found existing installation: smoothquant 0.0.0\n",
            "    Uninstalling smoothquant-0.0.0:\n",
            "      Successfully uninstalled smoothquant-0.0.0\n",
            "  Running setup.py develop for smoothquant\n",
            "Successfully installed smoothquant-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1188LC3LtZYt",
        "outputId": "fbe8b0c8-7d36-4b1b-d141-46dac43f540a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Guangxuan-Xiao/torch-int.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4AytQdgi_gj",
        "outputId": "1f9a29d3-c33a-46c0-9b6f-d9f34b821d9f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-int'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1102 (delta 139), reused 118 (delta 118), pack-reused 929\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 960.91 KiB | 6.91 MiB/s, done.\n",
            "Resolving deltas: 100% (643/643), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torch-int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjJqyK7jwSP",
        "outputId": "152abb09-f209-4994-d67b-edebf616b110"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also had to jump through some hoops trying to figure out this submodule stuff—it is not exactly what the repo says to do"
      ],
      "metadata": {
        "id": "FbQYofhC94cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git config submodule.submodules/cutlass.url https://github.com/NVIDIA/cutlass.git\n",
        "! git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI103aFzp_RG",
        "outputId": "38b6d0e4-e322-4c17-8f4e-28e7bed13cfa"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/torch-int/submodules/cutlass'...\n",
            "Submodule path 'submodules/cutlass': checked out 'c975e2ccbb2dbf13024568b37ffa3498ed0b3aed'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display cuda\n",
        "\n",
        "!sudo update-alternatives --config cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2k5e63-Uwg",
        "outputId": "4702f5db-b800-4f42-a9ca-e9d1f584190f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda - manual mode\n",
            "  link best version is /usr/local/cuda-12.2\n",
            "  link currently points to /usr/local/cuda-11.3\n",
            "  link cuda is /usr/local/cuda\n",
            "/usr/local/cuda-11.3 - priority 113\n",
            "/usr/local/cuda-12.2 - priority 122\n",
            "There are 2 choices for the alternative cuda (providing /usr/local/cuda).\n",
            "\n",
            "  Selection    Path                  Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/local/cuda-12.2   122       auto mode\n",
            "* 1            /usr/local/cuda-11.3   113       manual mode\n",
            "  2            /usr/local/cuda-12.2   122       manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2R7WxtNCBYZ",
        "outputId": "d6056029-8a82-460c-b896-a238a8ed1bd4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yet, when I try to build_cutlass.sh, the CUDA compiler is still identified as 12.2.140. The way to fix this is the cell above, changing the alternatives for CUDA usage"
      ],
      "metadata": {
        "id": "oREC2Ch-9xRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display gcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1zmiVBCElD",
        "outputId": "b5a8cb44-fe58-46aa-9e76-c338f3d444ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc - auto mode\n",
            "  link best version is /usr/bin/gcc-9\n",
            "  link currently points to /usr/bin/gcc-9\n",
            "  link gcc is /usr/bin/gcc\n",
            "  slave g++ is /usr/bin/g++\n",
            "/usr/bin/gcc-9 - priority 90\n",
            "  slave g++: /usr/bin/g++-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fixed it again! yay!\n",
        "\n",
        "os.environ['CC'] = '/usr/bin/gcc-9'\n",
        "os.environ['CXX'] = '/usr/bin/g++-9'\n",
        "os.environ['CUDAHOSTCXX'] = '/usr/bin/g++-9'"
      ],
      "metadata": {
        "id": "yYnMcbidHsgR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This still gave me errors for a bit, because you need to use a GCC compatible with the different CUDA version. But even when I installed a new GCC version, it seemed to use it but didn't? Weird."
      ],
      "metadata": {
        "id": "LM_FmyANCmtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! source environment.sh\n",
        "! CC=/usr/bin/gcc-9 CXX=/usr/bin/g++-9 bash build_cutlass.sh   # this one needs GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DSE8hTjCIU",
        "outputId": "4e730b03-0b83-42da-877d-664712755113",
        "collapsed": true
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- CMake Version: 3.30.1\n",
            "-- The CXX compiler identification is GNU 9.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 11.3.58\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- CUDART: /usr/local/cuda/lib64/libcudart.so\n",
            "-- CUDA Driver: /usr/local/cuda/lib64/stubs/libcuda.so\n",
            "-- NVRTC: /usr/local/cuda/lib64/libnvrtc.so\n",
            "-- Default Install Location: install\n",
            "-- CUDA Compilation Architectures: 80\n",
            "-- Enable caching of reference results in conv unit tests\n",
            "-- Enable rigorous conv problem sizes in conv unit tests\n",
            "-- Using NVCC flags: -DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;$<$<BOOL:1>:-Xcompiler=-Wconversion>;$<$<BOOL:1>:-Xcompiler=-fno-strict-aliasing>\n",
            "-- CUTLASS Revision: c975e2cc\n",
            "-- Configuring cublas ...\n",
            "-- cuBLAS Disabled.\n",
            "-- Configuring cuBLAS ... done.\n",
            "-- Found Python3: /usr/local/bin/python (found suitable version \"3.10.12\", minimum required is \"3.5\") found components: Interpreter\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c09b164b76dc.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.1134c512275c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.70bca42e032c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7412477fb00b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7cb784ae67c3.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.452e17cedfde.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.21cd46163fea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.f2656f9851f5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.da510d6f1a23.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed2bb5981d55.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.22f9e56a890e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.17417f539ca2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.d1a5010bad63.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5a217a522f90.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e764a5c32d2e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.00a490a48b54.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.67748a1f69fd.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.aaae24350764.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a7556fb0c363.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.cdd7679105ea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5b5abd9da9bb.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0188c1e5012f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c1136defa56d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.4efc263febd5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b1c066c89ad1.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.059188bf387f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dc6fdf4e3631.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed319cdaa486.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7f69d119038b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ab4c8725d89a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.db0920d13551.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.53863dce2b34.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5206989612aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a40fdb431023.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.999493302618.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.bbd766ae1258.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.3aca44f87dc7.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.fe5af8d45b59.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2d9ae5d87c7a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8112247fe822.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.447ac2531506.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b7cdd3426b6c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b83304556123.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c2bae868a773.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.51d0f833ea6d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dd6b23b6f933.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.eac06d5a86ae.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ec9d28d7c560.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.25dee0d0208a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.41f602329ae6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.9763c0db7135.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2fdf36a298c6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0a8164c28c5f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a4cfc29c733f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8a2ca8aaf7aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.977a56f2bc38.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e837fe821ee2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.6f06994db371.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ae96169b4e45.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a6a0ceb84086.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.19976851ec6d.cu\n",
            "-- Configuring done (3.2s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/torch-int/submodules/cutlass/build\n",
            "[  0%] \u001b[32mBuilding CUDA object examples/00_basic_gemm/CMakeFiles/00_basic_gemm.dir/basic_gemm.cu.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CUDA object examples/01_cutlass_utilities/CMakeFiles/01_cutlass_utilities.dir/cutlass_utilities.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/02_dump_reg_shmem/CMakeFiles/02_dump_reg_shmem.dir/dump_reg_shmem.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/visualize_layout.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/04_tile_iterator/CMakeFiles/04_tile_iterator.dir/tile_iterator.cu.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CUDA object examples/06_splitK_gemm/CMakeFiles/06_splitK_gemm.dir/splitk_gemm.cu.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object examples/07_volta_tensorop_gemm/CMakeFiles/07_volta_tensorop_gemm.dir/volta_tensorop_gemm.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object examples/08_turing_tensorop_gemm/CMakeFiles/08_turing_tensorop_gemm.dir/turing_tensorop_gemm.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/handle.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object examples/05_batched_gemm/CMakeFiles/05_batched_gemm.dir/batched_gemm.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/12_gemm_bias_relu/CMakeFiles/12_gemm_bias_relu.dir/gemm_bias_relu.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/09_turing_tensorop_conv2dfprop/CMakeFiles/09_turing_tensorop_conv2dfprop.dir/turing_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_rf.dir/fused_two_convs_f16_sm75_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_shmem.dir/fused_two_convs_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_rf.dir/fused_two_convs_f16_sm80_rf.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_shmem.dir/fused_two_convs_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/register_layout.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/src/manifest.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CUDA executable 04_tile_iterator\u001b[0m\n",
            "[  9%] Built target 04_tile_iterator\n",
            "[ 10%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/operation_table.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_rf.dir/fused_two_convs_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CUDA executable 02_dump_reg_shmem\u001b[0m\n",
            "[ 11%] Built target 02_dump_reg_shmem\n",
            "[ 12%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_shmem.dir/fused_two_convs_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32m\u001b[1mLinking CUDA executable 00_basic_gemm\u001b[0m\n",
            "[ 12%] Built target 00_basic_gemm\n",
            "[ 13%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_rf.dir/fused_two_convs_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/singleton.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CUDA executable 08_turing_tensorop_gemm\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CUDA executable 12_gemm_bias_relu\u001b[0m\n",
            "[ 14%] Built target 08_turing_tensorop_gemm\n",
            "[ 15%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_shmem.dir/fused_two_convs_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 15%] Built target 12_gemm_bias_relu\n",
            "[ 15%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_rf.dir/fused_two_gemms_f16_sm75_rf.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX executable 03_visualize_layout\u001b[0m\n",
            "[ 16%] Built target 03_visualize_layout\n",
            "[ 16%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_shmem.dir/fused_two_gemms_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CUDA executable 09_turing_tensorop_conv2dfprop\u001b[0m\n",
            "[ 16%] Built target 09_turing_tensorop_conv2dfprop\n",
            "[ 16%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_rf.dir/fused_two_gemms_f16_sm80_rf.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CUDA executable 01_cutlass_utilities\u001b[0m\n",
            "[ 16%] Built target 01_cutlass_utilities\n",
            "[ 16%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_shmem.dir/fused_two_gemms_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CUDA executable 05_batched_gemm\u001b[0m\n",
            "[ 16%] Built target 05_batched_gemm\n",
            "[ 16%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_rf.dir/fused_two_gemms_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/util.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_rf\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_rf\u001b[0m\n",
            "[ 18%] Built target 13_fused_two_convs_f16_sm80_rf\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_shmem.dir/fused_two_gemms_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/gemm.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_shmem\u001b[0m\n",
            "[ 19%] Built target 13_fused_two_convs_f16_sm75_rf\n",
            "[ 20%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_rf.dir/fused_two_gemms_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 20%] Built target 13_fused_two_convs_f16_sm80_shmem\n",
            "[ 21%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_shmem.dir/fused_two_gemms_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_shmem\u001b[0m\n",
            "[ 22%] Built target 13_fused_two_convs_f16_sm75_shmem\n",
            "[ 23%] \u001b[32mBuilding CUDA object examples/14_ampere_tf32_tensorop_gemm/CMakeFiles/14_ampere_tf32_tensorop_gemm.dir/ampere_tf32_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_rf\u001b[0m\n",
            "[ 23%] Built target 13_fused_two_convs_s8_sm75_rf\n",
            "[ 24%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/initialize_reference_operations.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_shmem\u001b[0m\n",
            "[ 24%] Built target 13_fused_two_convs_s8_sm75_shmem\n",
            "[ 25%] \u001b[32mBuilding CUDA object examples/15_ampere_sparse_tensorop_gemm/CMakeFiles/15_ampere_sparse_tensorop_gemm.dir/ampere_sparse_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_rf\u001b[0m\n",
            "[ 26%] Built target 13_fused_two_gemms_f16_sm75_rf\n",
            "[ 26%] \u001b[32mBuilding CUDA object examples/16_ampere_tensorop_conv2dfprop/CMakeFiles/16_ampere_tensorop_conv2dfprop.dir/ampere_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_shmem\u001b[0m\n",
            "[ 27%] Built target 13_fused_two_gemms_f16_sm75_shmem\n",
            "[ 27%] \u001b[32mBuilding CUDA object examples/17_fprop_per_channel_bias/CMakeFiles/17_fprop_per_channel_bias.dir/fprop_per_channel_bias.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/reduction_device.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_rf\u001b[0m\n",
            "[ 28%] Built target 13_fused_two_gemms_f16_sm80_rf\n",
            "[ 29%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/init_reduction_operations.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_shmem\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_gemms_f16_sm80_shmem\n",
            "[ 30%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv2d.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_rf\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_gemms_s8_sm75_rf\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeFiles/18_ampere_fp64_tensorop_affine2_gemm.dir/ampere_fp64_tensorop_affine2_gemm.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_rf\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_convs_s8_sm80_rf\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/19_tensorop_canonical/CMakeFiles/19_tensorop_canonical.dir/tensorop_canonical.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_shmem\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv3d.cu.o\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_convs_s8_sm80_shmem\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 14_ampere_tf32_tensorop_gemm\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/20_simt_canonical/CMakeFiles/20_simt_canonical.dir/simt_canonical.cu.o\u001b[0m\n",
            "[ 30%] Built target 14_ampere_tf32_tensorop_gemm\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/21_quaternion_gemm/CMakeFiles/21_quaternion_gemm.dir/quaternion_gemm.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/generated/initialize_all.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c09b164b76dc.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_shmem\u001b[0m\n",
            "[ 31%] Built target 13_fused_two_gemms_s8_sm75_shmem\n",
            "[ 32%] \u001b[32mBuilding CUDA object examples/22_quaternion_conv/CMakeFiles/22_quaternion_conv.dir/quaternion_conv.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 19_tensorop_canonical\u001b[0m\n",
            "[ 33%] Built target 19_tensorop_canonical\n",
            "[ 34%] \u001b[32mBuilding CUDA object examples/23_ampere_gemm_operand_reduction_fusion/CMakeFiles/23_ampere_gemm_operand_reduction_fusion.dir/ampere_gemm_operand_reduction_fusion.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_rf\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_shmem\u001b[0m\n",
            "[ 34%] Built target 13_fused_two_gemms_s8_sm80_shmem\n",
            "[ 35%] \u001b[32mBuilding CUDA object examples/24_gemm_grouped/CMakeFiles/24_gemm_grouped.dir/gemm_grouped.cu.o\u001b[0m\n",
            "[ 35%] Built target 13_fused_two_gemms_s8_sm80_rf\n",
            "[ 36%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_fprop_mainloop_fusion.dir/ampere_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA executable 15_ampere_sparse_tensorop_gemm\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CUDA executable 20_simt_canonical\u001b[0m\n",
            "[ 37%] Built target 15_ampere_sparse_tensorop_gemm\n",
            "[ 38%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_3d_fprop_mainloop_fusion.dir/ampere_3d_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 38%] Built target 20_simt_canonical\n",
            "[ 38%] \u001b[32mBuilding CUDA object examples/26_ampere_wgrad_mainloop_fusion/CMakeFiles/26_ampere_wgrad_mainloop_fusion.dir/ampere_wgrad_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CUDA executable 16_ampere_tensorop_conv2dfprop\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CUDA executable 17_fprop_per_channel_bias\u001b[0m\n",
            "[ 40%] Built target 16_ampere_tensorop_conv2dfprop\n",
            "[ 41%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.1134c512275c.cu.o\u001b[0m\n",
            "[ 41%] Built target 17_fprop_per_channel_bias\n",
            "[ 41%] \u001b[32mBuilding CUDA object examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeFiles/27_ampere_3xtf32_fast_accurate_tensorop_gemm.dir/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 18_ampere_fp64_tensorop_affine2_gemm\u001b[0m\n",
            "[ 42%] Built target 18_ampere_fp64_tensorop_affine2_gemm\n",
            "[ 42%] \u001b[32mBuilding CUDA object examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeFiles/28_ampere_3xtf32_fast_accurate_tensorop_fprop.dir/ampere_3xtf32_fast_accurate_tensorop_fprop.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 21_quaternion_gemm\u001b[0m\n",
            "[ 42%] Built target 21_quaternion_gemm\n",
            "[ 42%] \u001b[32mBuilding CUDA object examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeFiles/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.dir/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 23_ampere_gemm_operand_reduction_fusion\u001b[0m\n",
            "[ 42%] Built target 23_ampere_gemm_operand_reduction_fusion\n",
            "[ 42%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.70bca42e032c.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_fprop_mainloop_fusion\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 22_quaternion_conv\u001b[0m\n",
            "[ 42%] Built target 25_ampere_fprop_mainloop_fusion\n",
            "[ 42%] \u001b[32mBuilding CUDA object examples/30_wgrad_split_k/CMakeFiles/30_wgrad_split_k.dir/30_wgrad_split_k.cu.o\u001b[0m\n",
            "[ 42%] Built target 22_quaternion_conv\n",
            "[ 43%] \u001b[32mBuilding CUDA object examples/31_basic_syrk/CMakeFiles/31_basic_syrk.dir/basic_syrk.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_3d_fprop_mainloop_fusion\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 26_ampere_wgrad_mainloop_fusion\u001b[0m\n",
            "[ 44%] Built target 25_ampere_3d_fprop_mainloop_fusion\n",
            "[ 45%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7412477fb00b.cu.o\u001b[0m\n",
            "[ 45%] Built target 26_ampere_wgrad_mainloop_fusion\n",
            "[ 46%] \u001b[32mBuilding CUDA object examples/32_basic_trmm/CMakeFiles/32_basic_trmm.dir/basic_trmm.cu.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CUDA executable 27_ampere_3xtf32_fast_accurate_tensorop_gemm\u001b[0m\n",
            "[ 47%] Built target 27_ampere_3xtf32_fast_accurate_tensorop_gemm\n",
            "[ 48%] \u001b[32mBuilding CUDA object examples/33_ampere_3xtf32_tensorop_symm/CMakeFiles/33_ampere_3xtf32_tensorop_symm.dir/ampere_3xtf32_tensorop_symm.cu.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CUDA executable 28_ampere_3xtf32_fast_accurate_tensorop_fprop\u001b[0m\n",
            "[ 49%] Built target 28_ampere_3xtf32_fast_accurate_tensorop_fprop\n",
            "[ 49%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7cb784ae67c3.cu.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CUDA executable 31_basic_syrk\u001b[0m\n",
            "[ 49%] Built target 31_basic_syrk\n",
            "[ 50%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.452e17cedfde.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable 32_basic_trmm\u001b[0m\n",
            "[ 50%] Built target 32_basic_trmm\n",
            "[ 50%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.21cd46163fea.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable 24_gemm_grouped\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable 30_wgrad_split_k\u001b[0m\n",
            "[ 50%] Built target 30_wgrad_split_k\n",
            "[ 51%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.f2656f9851f5.cu.o\u001b[0m\n",
            "[ 51%] Built target 24_gemm_grouped\n",
            "[ 52%] \u001b[32mBuilding CUDA object examples/34_transposed_conv2d/CMakeFiles/34_transposed_conv2d.dir/34_transposed_conv2d.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/35_gemm_softmax/CMakeFiles/35_gemm_softmax.dir/gemm_softmax.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/36_gather_scatter_fusion/CMakeFiles/36_gather_scatter_fusion.dir/gather_scatter_fusion.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\u001b[0m\n",
            "[ 54%] Built target 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\n",
            "[ 54%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.da510d6f1a23.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 06_splitK_gemm\u001b[0m\n",
            "[ 54%] Built target 06_splitK_gemm\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 33_ampere_3xtf32_tensorop_symm\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CUDA object examples/37_gemm_layernorm_gemm_fusion/CMakeFiles/37_gemm_layernorm_gemm_fusion.dir/gemm_layernorm.cu.o\u001b[0m\n",
            "[ 54%] Built target 33_ampere_3xtf32_tensorop_symm\n",
            "[ 54%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed2bb5981d55.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 07_volta_tensorop_gemm\u001b[0m\n",
            "[ 54%] Built target 07_volta_tensorop_gemm\n",
            "[ 54%] \u001b[32mBuilding CUDA object examples/38_syr2k_grouped/CMakeFiles/38_syr2k_grouped.dir/syr2k_grouped.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 34_transposed_conv2d\u001b[0m\n",
            "[ 54%] Built target 34_transposed_conv2d\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(507): warning: variable \"minus\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "[ 54%] \u001b[32mBuilding CUDA object examples/39_gemm_permute/CMakeFiles/39_gemm_permute.dir/gemm_permute.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(508): warning: variable \"mul\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(509): warning: variable \"exponential\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 35_gemm_softmax\u001b[0m\n",
            "[ 54%] Built target 35_gemm_softmax\n",
            "[ 54%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_fixed_seqlen.dir/fused_multihead_attention_fixed_seqlen.cu.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable 36_gather_scatter_fusion\u001b[0m\n",
            "[ 55%] Built target 36_gather_scatter_fusion\n",
            "[ 56%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.22f9e56a890e.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1083): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CUDA executable 37_gemm_layernorm_gemm_fusion\u001b[0m\n",
            "[ 57%] Built target 37_gemm_layernorm_gemm_fusion\n",
            "[ 57%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_variable_seqlen.dir/fused_multihead_attention_variable_seqlen.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CUDA executable 39_gemm_permute\u001b[0m\n",
            "[ 58%] Built target 39_gemm_permute\n",
            "[ 59%] \u001b[32mBuilding CUDA object examples/42_ampere_tensorop_group_conv/CMakeFiles/42_ampere_tensorop_group_conv.dir/ampere_tensorop_group_conv.cu.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.17417f539ca2.cu.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.d1a5010bad63.cu.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5a217a522f90.cu.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CUDA executable 38_syr2k_grouped\u001b[0m\n",
            "[ 61%] Built target 38_syr2k_grouped\n",
            "[ 62%] \u001b[32mBuilding CUDA object examples/43_ell_block_sparse_gemm/CMakeFiles/43_ell_block_sparse_gemm.dir/ell_block_sparse_gemm.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e764a5c32d2e.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.00a490a48b54.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CUDA executable 42_ampere_tensorop_group_conv\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.67748a1f69fd.cu.o\u001b[0m\n",
            "[ 64%] Built target 42_ampere_tensorop_group_conv\n",
            "[ 65%] \u001b[32mBuilding CUDA object examples/45_dual_gemm/CMakeFiles/45_dual_gemm.dir/dual_gemm.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.aaae24350764.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_fixed_seqlen\u001b[0m\n",
            "[ 66%] Built target 41_fused_multi_head_attention_fixed_seqlen\n",
            "[ 67%] \u001b[32mBuilding CUDA object examples/46_depthwise_simt_conv2dfprop/CMakeFiles/46_depthwise_simt_conv2dfprop.dir/depthwise_simt_conv2dfprop.cu.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CUDA executable 43_ell_block_sparse_gemm\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a7556fb0c363.cu.o\u001b[0m\n",
            "[ 68%] Built target 43_ell_block_sparse_gemm\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.cdd7679105ea.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5b5abd9da9bb.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CUDA executable 46_depthwise_simt_conv2dfprop\u001b[0m\n",
            "[ 68%] Built target 46_depthwise_simt_conv2dfprop\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0188c1e5012f.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CUDA executable 45_dual_gemm\u001b[0m\n",
            "[ 69%] Built target 45_dual_gemm\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c1136defa56d.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.4efc263febd5.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b1c066c89ad1.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_variable_seqlen\u001b[0m\n",
            "[ 70%] Built target 41_fused_multi_head_attention_variable_seqlen\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.059188bf387f.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dc6fdf4e3631.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed319cdaa486.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7f69d119038b.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ab4c8725d89a.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.db0920d13551.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.53863dce2b34.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5206989612aa.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a40fdb431023.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.999493302618.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.bbd766ae1258.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.3aca44f87dc7.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.fe5af8d45b59.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2d9ae5d87c7a.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8112247fe822.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.447ac2531506.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b7cdd3426b6c.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b83304556123.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c2bae868a773.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.51d0f833ea6d.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dd6b23b6f933.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.eac06d5a86ae.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ec9d28d7c560.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.25dee0d0208a.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.41f602329ae6.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.9763c0db7135.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2fdf36a298c6.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0a8164c28c5f.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a4cfc29c733f.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8a2ca8aaf7aa.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.977a56f2bc38.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e837fe821ee2.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.6f06994db371.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ae96169b4e45.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a6a0ceb84086.cu.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.19976851ec6d.cu.o\u001b[0m\n",
            "[ 87%] Built target cutlass_library_objs\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX shared library libcutlass.so\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX static library libcutlass.a\u001b[0m\n",
            "[ 88%] Built target cutlass_library_static\n",
            "[ 88%] Built target cutlass_lib\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/10_planar_complex/CMakeFiles/10_planar_complex.dir/planar_complex.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/11_planar_complex_array/CMakeFiles/11_planar_complex_array.dir/planar_complex_array.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cutlass_profiler.cu.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/options.cu.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/performance_report.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/enumerated_types.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gpu_timer.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_allocation.cu.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_context.cu.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cublas_helpers.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cudnn_helpers.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/problem_space.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/operation_profiler.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_k_operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_2k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/trmm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/symm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv2d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv3d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/sparse_gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CUDA executable 11_planar_complex_array\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CUDA executable 10_planar_complex\u001b[0m\n",
            "[ 99%] Built target 11_planar_complex_array\n",
            "[ 99%] Built target 10_planar_complex\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable cutlass_profiler\u001b[0m\n",
            "[100%] Built target cutlass_profiler\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing torch_int.egg-info/PKG-INFO\n",
            "writing dependency_links to torch_int.egg-info/dependency_links.txt\n",
            "writing top-level names to torch_int.egg-info/top_level.txt\n",
            "reading manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying torch_int/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int\n",
            "copying torch_int/models/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/models/opt.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/nn/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/linear.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/quantization.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/utils\n",
            "running build_ext\n",
            "building 'torch_int._CUDA' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/torch_int\n",
            "creating build/temp.linux-x86_64-cpython-310/torch_int/kernels\n",
            "/usr/bin/gcc-9 -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -Itorch_int/kernels/include -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bindings.cpp -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bindings.o -std=c++14 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda-11.3/bin/nvcc -Itorch_int/kernels/include -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bmm.cu -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DCUDA_ARCH=800 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -ccbin /usr/bin/gcc-9\n",
            "\u001b[01m\u001b[Ktorch_int/kernels/bmm.cu:3:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kcutlass/core_io.h: No such file or directory\n",
            "    3 | #include \u001b[01;31m\u001b[K<cutlass/core_io.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "error: command '/usr/local/cuda-11.3/bin/nvcc' failed with exit code 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf /content/torch-int/build  # Or the appropriate build directory"
      ],
      "metadata": {
        "id": "RvyR2v1rSJOO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfpQCEdrKGf8",
        "outputId": "b07bb119-b1f0-47d8-b18b-ce55c7cd2adc"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing torch_int.egg-info/PKG-INFO\n",
            "writing dependency_links to torch_int.egg-info/dependency_links.txt\n",
            "writing top-level names to torch_int.egg-info/top_level.txt\n",
            "reading manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying torch_int/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int\n",
            "copying torch_int/models/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/models/opt.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/nn/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/linear.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/quantization.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/utils\n",
            "running build_ext\n",
            "building 'torch_int._CUDA' extension\n",
            "/usr/bin/gcc-9 -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/torch-int/submodules/cutlass/tools/util/include/cutlass/util/ -I/content/torch-int/submodules/cutlass/include/cutlass/ -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bindings.cpp -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bindings.o -std=c++14 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda-11.3/bin/nvcc -I/content/torch-int/submodules/cutlass/tools/util/include/cutlass/util/ -I/content/torch-int/submodules/cutlass/include/cutlass/ -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bmm.cu -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DCUDA_ARCH=800 -I/content/torch-int/torch-int/cutlass/include -I/content/torch-int/torch-int/cutlass/tools/util/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -ccbin /usr/bin/gcc-9\n",
            "In file included from \u001b[01m\u001b[Ktorch_int/kernels/bmm.cu:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/torch-int/submodules/cutlass/include/cutlass/core_io.h:40:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kcutlass/array.h: No such file or directory\n",
            "   40 | #include \u001b[01;31m\u001b[K\"cutlass/array.h\"\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "error: command '/usr/local/cuda-11.3/bin/nvcc' failed with exit code 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3jndfsRlcKUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import smoothquant"
      ],
      "metadata": {
        "id": "FCA9Lpz6fi2u"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smoothquant.opt import Int8OPTForCausalLM\n",
        "model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "DKYIiWlbgfsq",
        "outputId": "61e6cce0-346d-43c6-8d5c-8332c45cbf17"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'smoothquant.opt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4c536754c939>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mit-han-lab/opt-30b-smoothquant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smoothquant.opt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5x-01auil_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}