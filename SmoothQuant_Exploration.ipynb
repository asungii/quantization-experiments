{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNKsCGrUPwItWtL6bLqGxeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asungii/quantization-experiments/blob/main/SmoothQuant_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Installations</h1>\n",
        "\n",
        "From SmoothQuant repo. Because the specified CUDA compiler is 11.3, and the default Colab CUDA version is as of 7/29/24 12.2, we have to change the CUDA version in Colab.\n",
        "\n",
        "I found info on how to do this from:\n",
        "\n",
        "https://medium.com/@ajithkumarv/how-to-modify-cuda-gcc-python-versions-in-colab-584ed4113157\n",
        "\n",
        "https://github.com/googlecolab/colabtools/issues/4214#issuecomment-1859155789\n",
        "\n",
        "https://vitalitylearning.medium.com/running-cuda-in-google-colab-525a92efcf75"
      ],
      "metadata": {
        "id": "nKLSnmrnyb2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6NI2WxlYpUM",
        "outputId": "66f84bb2-7e34-4519-e7bf-4ccee9f675f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.7.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers==4.36.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, xxhash, requests, pyarrow, fsspec, dill, multiprocess, tokenizers, transformers, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 tokenizers-0.15.2 transformers-4.36.0 xxhash-3.4.1 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "# setup is from the mit-han-lab GitHub\n",
        "\n",
        "# these bits are not necessary for Google Colab\n",
        "# !conda create -n smoothquant python=3.8\n",
        "# !conda activate smoothquant\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers==4.36.0 accelerate datasets zstandard\n",
        "\n",
        "# !python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "45Rh36UA8efT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "\n",
        "# well it's 12.2. that's a bit of an issue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azoig1mPz7bV",
        "outputId": "b99a1ea5-0066-4382-f59e-6748250fe7e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
        "!sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.3\n",
        "\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrTVEc6byXAs",
        "outputId": "abdc7dbb-97b7-4c07-fdc7-a807c4739df5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-29 14:29:49--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu2004.pin’\n",
            "\n",
            "\rcuda-ubuntu2004.pin   0%[                    ]       0  --.-KB/s               \rcuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-29 14:29:49 (5.84 MB/s) - ‘cuda-ubuntu2004.pin’ saved [190/190]\n",
            "\n",
            "--2024-07-29 14:29:49--  https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2286573544 (2.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu200 100%[===================>]   2.13G   205MB/s    in 11s     \n",
            "\n",
            "2024-07-29 14:30:01 (190 MB/s) - ‘cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb’ saved [2286573544/2286573544]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu2004-11-3-local.\n",
            "(Reading database ... 123589 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "Setting up cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "\n",
            "The public CUDA GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 file:/var/cuda-repo-ubuntu2004-11-3-local  Packages [30.3 kB]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,549 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,208 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,129 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,390 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,877 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Fetched 19.0 MB in 2s (9,261 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-11.3-1' for regex 'cuda-11.3'\n",
            "Note, selecting 'cuda-11-3' for regex 'cuda-11.3'\n",
            "The following additional packages will be installed:\n",
            "  cpp-12 cuda-command-line-tools-11-3 cuda-compiler-11-3 cuda-cudart-11-3\n",
            "  cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3 cuda-cupti-dev-11-3\n",
            "  cuda-cuxxfilt-11-3 cuda-demo-suite-11-3 cuda-documentation-11-3\n",
            "  cuda-driver-dev-11-3 cuda-drivers cuda-drivers-555 cuda-gdb-11-3\n",
            "  cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-555 libnvidia-common-555\n",
            "  libnvidia-compute-555 libnvidia-decode-555 libnvidia-encode-555\n",
            "  libnvidia-extra-555 libnvidia-fbc1-555 libnvidia-gl-555 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libudev1 libxcvt0 libxfont2\n",
            "  libxkbfile1 libxtst6 libxxf86dga1 nsight-compute-2021.1.0\n",
            "  nsight-systems-2021.1.3 nvidia-compute-utils-555 nvidia-dkms-555\n",
            "  nvidia-driver-555 nvidia-firmware-555-555.42.06 nvidia-kernel-common-555\n",
            "  nvidia-kernel-source-555 nvidia-prime nvidia-settings nvidia-utils-555\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-555\n",
            "Suggested packages:\n",
            "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc gvfs\n",
            "  mesa-utils xfs | xserver xfonts-100dpi | xfonts-75dpi xfonts-scalable\n",
            "Recommended packages:\n",
            "  libnvidia-compute-555:i386 libnvidia-decode-555:i386\n",
            "  libnvidia-encode-555:i386 libnvidia-fbc1-555:i386 libnvidia-gl-555:i386\n",
            "The following NEW packages will be installed:\n",
            "  cpp-12 cuda-11-3 cuda-command-line-tools-11-3 cuda-compiler-11-3\n",
            "  cuda-cudart-11-3 cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3\n",
            "  cuda-cupti-dev-11-3 cuda-cuxxfilt-11-3 cuda-demo-suite-11-3\n",
            "  cuda-documentation-11-3 cuda-driver-dev-11-3 cuda-drivers cuda-drivers-555\n",
            "  cuda-gdb-11-3 cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-555 libnvidia-common-555\n",
            "  libnvidia-compute-555 libnvidia-decode-555 libnvidia-encode-555\n",
            "  libnvidia-extra-555 libnvidia-fbc1-555 libnvidia-gl-555 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libxcvt0 libxfont2 libxkbfile1\n",
            "  libxtst6 libxxf86dga1 nsight-compute-2021.1.0 nsight-systems-2021.1.3\n",
            "  nvidia-compute-utils-555 nvidia-dkms-555 nvidia-driver-555\n",
            "  nvidia-firmware-555-555.42.06 nvidia-kernel-common-555\n",
            "  nvidia-kernel-source-555 nvidia-prime nvidia-settings nvidia-utils-555\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-555\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 115 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 325 MB/2,361 MB of archives.\n",
            "After this operation, 5,413 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-11-3 11.3.58-1 [156 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-config-common 11.8.89-1 [16.4 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-555 555.42.06-0ubuntu1 [17.2 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-555 555.42.06-0ubuntu1 [46.9 MB]\n",
            "Get:5 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-11-3 11.3.58-1 [25.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-555 555.42.06-0ubuntu1 [138 MB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
            "Get:10 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-11-3 11.4.2.10064-1 [134 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-555 555.42.06-0ubuntu1 [41.4 MB]\n",
            "Get:16 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-11-3 10.4.2.58-1 [107 MB]\n",
            "Get:17 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-11-3 10.2.4.58-1 [40.0 MB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-555-555.42.06 555.42.06-0ubuntu1 [36.5 MB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.11 [1,477 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.23+9-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-555 555.42.06-0ubuntu1 [109 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-555 555.42.06-0ubuntu1 [36.2 kB]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-555 555.42.06-0ubuntu1 [73.4 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-555 555.42.06-0ubuntu1 [118 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-555 555.42.06-0ubuntu1 [1,792 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-555 555.42.06-0ubuntu1 [104 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-555 555.42.06-0ubuntu1 [496 kB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-555 555.42.06-0ubuntu1 [146 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-555 555.42.06-0ubuntu1 [1,534 kB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-555 555.42.06-0ubuntu1 [75.8 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-555 555.42.06-0ubuntu1 [490 kB]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-555 555.42.06-1 [2,546 B]\n",
            "Get:51 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-11-3 11.1.1.58-1 [75.7 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 555.42.06-1 [2,506 B]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 555.42.06-0ubuntu1 [946 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 nvidia-prime all 0.8.17.1 [9,956 B]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:72 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-11-3 11.5.0.58-1 [99.1 MB]\n",
            "Get:73 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-11-3 11.3.3.44-1 [72.3 MB]\n",
            "Get:74 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-11-3 11.4.1.58-1 [1,736 kB]\n",
            "Get:75 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-11-3 11.3.0-1 [2,502 B]\n",
            "Get:76 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-runtime-11-3 11.3.0-1 [2,420 B]\n",
            "Get:77 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuobjdump-11-3 11.3.58-1 [112 kB]\n",
            "Get:78 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuxxfilt-11-3 11.3.58-1 [44.1 kB]\n",
            "Get:79 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-thrust-11-3 11.3.58-1 [982 kB]\n",
            "Get:80 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-driver-dev-11-3 11.3.58-1 [26.2 kB]\n",
            "Get:81 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-dev-11-3 11.3.58-1 [737 kB]\n",
            "Get:82 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvcc-11-3 11.3.58-1 [45.7 MB]\n",
            "Get:83 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprune-11-3 11.3.58-1 [54.9 kB]\n",
            "Get:84 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-compiler-11-3 11.3.0-1 [2,428 B]\n",
            "Get:85 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-dev-11-3 11.3.58-1 [23.3 kB]\n",
            "Get:86 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-dev-11-3 11.4.2.10064-1 [141 MB]\n",
            "Get:87 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-dev-11-3 10.4.2.58-1 [179 MB]\n",
            "Get:88 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-dev-11-3 10.2.4.58-1 [40.4 MB]\n",
            "Get:89 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-dev-11-3 11.1.1.58-1 [21.5 MB]\n",
            "Get:90 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-dev-11-3 11.5.0.58-1 [99.8 MB]\n",
            "Get:91 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-dev-11-3 11.3.3.44-1 [70.4 MB]\n",
            "Get:92 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-dev-11-3 11.4.1.58-1 [1,428 kB]\n",
            "Get:93 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-dev-11-3 11.3.0-1 [2,530 B]\n",
            "Get:94 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-11-3 11.3.58-1 [11.5 MB]\n",
            "Get:95 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-dev-11-3 11.3.58-1 [2,401 kB]\n",
            "Get:96 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvdisasm-11-3 11.3.58-1 [32.9 MB]\n",
            "Get:97 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-gdb-11-3 11.3.58-1 [3,622 kB]\n",
            "Get:98 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-memcheck-11-3 11.3.58-1 [145 kB]\n",
            "Get:99 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprof-11-3 11.3.58-1 [1,930 kB]\n",
            "Get:100 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvtx-11-3 11.3.58-1 [51.1 kB]\n",
            "Get:101 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-sanitizer-11-3 11.3.58-1 [7,534 kB]\n",
            "Get:102 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-command-line-tools-11-3 11.3.0-1 [2,466 B]\n",
            "Get:103 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-compute-2021.1.0 2021.1.0.18-1 [274 MB]\n",
            "Get:104 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-compute-11-3 11.3.0-1 [3,708 B]\n",
            "Get:105 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-systems-2021.1.3 2021.1.3.14-b695ea9 [248 MB]\n",
            "Get:106 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-systems-11-3 11.3.0-1 [3,302 B]\n",
            "Get:107 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-11-3 11.3.58-1 [119 MB]\n",
            "Get:108 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvml-dev-11-3 11.3.58-1 [73.3 kB]\n",
            "Get:109 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvvp-11-3 11.3.58-1 [115 MB]\n",
            "Get:110 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-visual-tools-11-3 11.3.0-1 [2,868 B]\n",
            "Get:111 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-tools-11-3 11.3.0-1 [2,380 B]\n",
            "Get:112 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-samples-11-3 11.3.58-1 [59.2 MB]\n",
            "Get:113 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-documentation-11-3 11.3.58-1 [48.2 kB]\n",
            "Get:114 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-toolkit-11-3 11.3.0-1 [3,274 B]\n",
            "Get:115 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-demo-suite-11-3 11.3.58-1 [3,978 kB]\n",
            "Get:116 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-11-3 11.3.0-1 [2,450 B]\n",
            "Fetched 325 MB in 16s (19.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 116.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package liblocale-gettext-perl.\n",
            "(Reading database ... 123701 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
            "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
            "Selecting previously unselected package keyboard-configuration.\n",
            "Preparing to unpack .../1-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
            "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
            "Selecting previously unselected package cpp-12.\n",
            "Preparing to unpack .../2-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan8:amd64.\n",
            "Preparing to unpack .../3-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libtsan2:amd64.\n",
            "Preparing to unpack .../4-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-12-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-12.\n",
            "Preparing to unpack .../6-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package dctrl-tools.\n",
            "Preparing to unpack .../7-dctrl-tools_2.24-3build2_amd64.deb ...\n",
            "Unpacking dctrl-tools (2.24-3build2) ...\n",
            "Selecting previously unselected package dkms.\n",
            "Preparing to unpack .../8-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
            "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
            "Preparing to unpack .../9-libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 124061 files and directories currently installed.)\n",
            "Preparing to unpack .../000-udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "Preparing to unpack .../001-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-config-common.\n",
            "Preparing to unpack .../002-cuda-toolkit-11-config-common_11.8.89-1_all.deb ...\n",
            "Unpacking cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-11-3.\n",
            "Preparing to unpack .../003-cuda-cudart-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-3.\n",
            "Preparing to unpack .../004-cuda-nvrtc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-11-3.\n",
            "Preparing to unpack .../005-libcublas-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-11-3.\n",
            "Preparing to unpack .../006-libcufft-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-11-3.\n",
            "Preparing to unpack .../007-libcurand-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-11-3.\n",
            "Preparing to unpack .../008-libcusolver-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-11-3.\n",
            "Preparing to unpack .../009-libcusparse-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-11-3.\n",
            "Preparing to unpack .../010-libnpp-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-3.\n",
            "Preparing to unpack .../011-libnvjpeg-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-3.\n",
            "Preparing to unpack .../012-cuda-libraries-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libnvidia-common-555.\n",
            "Preparing to unpack .../013-libnvidia-common-555_555.42.06-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-555:amd64.\n",
            "Preparing to unpack .../014-libnvidia-compute-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-gl-555:amd64.\n",
            "Preparing to unpack .../015-libnvidia-gl-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-535\n",
            "Unpacking libnvidia-gl-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-source-555.\n",
            "Preparing to unpack .../016-nvidia-kernel-source-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-firmware-555-555.42.06.\n",
            "Preparing to unpack .../017-nvidia-firmware-555-555.42.06_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-firmware-555-555.42.06 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-common-555.\n",
            "Preparing to unpack .../018-nvidia-kernel-common-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-dkms-555.\n",
            "Preparing to unpack .../019-nvidia-dkms-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-dkms-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-extra-555:amd64.\n",
            "Preparing to unpack .../020-libnvidia-extra-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-extra-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-555.\n",
            "Preparing to unpack .../021-nvidia-compute-utils-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-decode-555:amd64.\n",
            "Preparing to unpack .../022-libnvidia-decode-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-555:amd64.\n",
            "Preparing to unpack .../023-libnvidia-encode-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-555.\n",
            "Preparing to unpack .../024-nvidia-utils-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-cfg1-555:amd64.\n",
            "Preparing to unpack .../025-libnvidia-cfg1-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../026-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../027-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../028-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package libxcvt0:amd64.\n",
            "Preparing to unpack .../029-libxcvt0_0.1.1-3_amd64.deb ...\n",
            "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../030-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../031-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package xserver-xorg-core.\n",
            "Preparing to unpack .../032-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xserver-xorg-video-nvidia-555.\n",
            "Preparing to unpack .../033-xserver-xorg-video-nvidia-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-fbc1-555:amd64.\n",
            "Preparing to unpack .../034-libnvidia-fbc1-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-555.\n",
            "Preparing to unpack .../035-nvidia-driver-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package cuda-drivers-555.\n",
            "Preparing to unpack .../036-cuda-drivers-555_555.42.06-1_amd64.deb ...\n",
            "Unpacking cuda-drivers-555 (555.42.06-1) ...\n",
            "Selecting previously unselected package cuda-drivers.\n",
            "Preparing to unpack .../037-cuda-drivers_555.42.06-1_amd64.deb ...\n",
            "Unpacking cuda-drivers (555.42.06-1) ...\n",
            "Selecting previously unselected package cuda-runtime-11-3.\n",
            "Preparing to unpack .../038-cuda-runtime-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-3.\n",
            "Preparing to unpack .../039-cuda-cuobjdump-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-11-3.\n",
            "Preparing to unpack .../040-cuda-cuxxfilt-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-thrust-11-3.\n",
            "Preparing to unpack .../041-cuda-thrust-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-3.\n",
            "Preparing to unpack .../042-cuda-driver-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-3.\n",
            "Preparing to unpack .../043-cuda-cudart-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-3.\n",
            "Preparing to unpack .../044-cuda-nvcc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-3.\n",
            "Preparing to unpack .../045-cuda-nvprune-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-3.\n",
            "Preparing to unpack .../046-cuda-compiler-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-3.\n",
            "Preparing to unpack .../047-cuda-nvrtc-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-3.\n",
            "Preparing to unpack .../048-libcublas-dev-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-3.\n",
            "Preparing to unpack .../049-libcufft-dev-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-3.\n",
            "Preparing to unpack .../050-libcurand-dev-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-3.\n",
            "Preparing to unpack .../051-libcusolver-dev-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-3.\n",
            "Preparing to unpack .../052-libcusparse-dev-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-3.\n",
            "Preparing to unpack .../053-libnpp-dev-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-3.\n",
            "Preparing to unpack .../054-libnvjpeg-dev-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-3.\n",
            "Preparing to unpack .../055-cuda-libraries-dev-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-3.\n",
            "Preparing to unpack .../056-cuda-cupti-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-3.\n",
            "Preparing to unpack .../057-cuda-cupti-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-3.\n",
            "Preparing to unpack .../058-cuda-nvdisasm-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-3.\n",
            "Preparing to unpack .../059-cuda-gdb-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-3.\n",
            "Preparing to unpack .../060-cuda-memcheck-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-3.\n",
            "Preparing to unpack .../061-cuda-nvprof-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-3.\n",
            "Preparing to unpack .../062-cuda-nvtx-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-3.\n",
            "Preparing to unpack .../063-cuda-sanitizer-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-3.\n",
            "Preparing to unpack .../064-cuda-command-line-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-compute-2021.1.0.\n",
            "Preparing to unpack .../065-nsight-compute-2021.1.0_2021.1.0.18-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-3.\n",
            "Preparing to unpack .../066-cuda-nsight-compute-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-systems-2021.1.3.\n",
            "Preparing to unpack .../067-nsight-systems-2021.1.3_2021.1.3.14-1_amd64.deb ...\n",
            "Unpacking nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-3.\n",
            "Preparing to unpack .../068-cuda-nsight-systems-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../069-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../070-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../071-openjdk-11-jre_11.0.23+9-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.23+9-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../072-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-11-3.\n",
            "Preparing to unpack .../073-cuda-nsight-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-3.\n",
            "Preparing to unpack .../074-cuda-nvml-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-3.\n",
            "Preparing to unpack .../075-cuda-nvvp-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-3.\n",
            "Preparing to unpack .../076-cuda-visual-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-3.\n",
            "Preparing to unpack .../077-cuda-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-samples-11-3.\n",
            "Preparing to unpack .../078-cuda-samples-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-samples-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-3.\n",
            "Preparing to unpack .../079-cuda-documentation-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-3.\n",
            "Preparing to unpack .../080-cuda-toolkit-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-11-3.\n",
            "Preparing to unpack .../081-cuda-demo-suite-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-11-3.\n",
            "Preparing to unpack .../082-cuda-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libfakeroot:amd64.\n",
            "Preparing to unpack .../083-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fakeroot.\n",
            "Preparing to unpack .../084-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../085-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../086-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../087-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../088-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../089-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../090-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../091-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../092-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../093-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../094-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../095-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../096-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package nvidia-prime.\n",
            "Preparing to unpack .../097-nvidia-prime_0.8.17.1_all.deb ...\n",
            "Unpacking nvidia-prime (0.8.17.1) ...\n",
            "Selecting previously unselected package python3-xkit.\n",
            "Preparing to unpack .../098-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
            "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
            "Selecting previously unselected package screen-resolution-extra.\n",
            "Preparing to unpack .../099-screen-resolution-extra_0.18.2_all.deb ...\n",
            "Unpacking screen-resolution-extra (0.18.2) ...\n",
            "Selecting previously unselected package nvidia-settings.\n",
            "Preparing to unpack .../100-nvidia-settings_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-settings (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../101-systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Selecting previously unselected package xcvt.\n",
            "Preparing to unpack .../102-xcvt_0.1.1-3_amd64.deb ...\n",
            "Unpacking xcvt (0.1.1-3) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../103-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../104-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../105-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Setting up libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Setting up libnvidia-common-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-fbc1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Setting up cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Setting up cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-cfg1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-prime (0.8.17.1) ...\n",
            "Setting up cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.23+9-1ubuntu1~22.04.1) ...\n",
            "Setting up libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Setting up cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Setting up libnvidia-extra-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up fakeroot (1.28-1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
            "Setting up cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-compute-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-firmware-555-555.42.06 (555.42.06-0ubuntu1) ...\n",
            "Setting up libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libnpp-11-3 (11.3.3.44-1) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Setting up libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Setting up nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: error: alternative path /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsight-sys doesn't exist\n",
            "update-alternatives: error: no alternatives for nsight-sys\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
            "Setting up cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libcurand-11-3 (10.2.4.58-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Setting up libcufft-11-3 (10.4.2.58-1) ...\n",
            "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
            "Setting up libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Setting up cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
            "Setting up dctrl-tools (2.24-3build2) ...\n",
            "Setting up libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Setting up nvidia-kernel-source-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Setting up nvidia-compute-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory\n",
            "Adding system user `nvidia-persistenced' (UID 104) ...\n",
            "Adding new group `nvidia-persistenced' (GID 111) ...\n",
            "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
            "Not creating home directory `/nonexistent'.\n",
            "Setting up cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Setting up cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-decode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up nvidia-kernel-common-555 (555.42.06-0ubuntu1) ...\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-hibernate.service → /lib/systemd/system/nvidia-hibernate.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-resume.service → /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-resume.service → /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-suspend.service → /lib/systemd/system/nvidia-suspend.service.\n",
            "Setting up cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Setting up xcvt (0.1.1-3) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up libnvidia-gl-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Setting up screen-resolution-extra (0.18.2) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up nvidia-settings (555.42.06-0ubuntu1) ...\n",
            "Setting up libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Configuring keyboard-configuration\n",
            "----------------------------------\n",
            "\n",
            "The layout of keyboards varies per country, with some countries having multiple\n",
            "common layouts. Please select the country of origin for the keyboard of this\n",
            "computer.\n",
            "\n",
            "  1. Afghani\n",
            "  2. Albanian\n",
            "  3. Amharic\n",
            "  4. Arabic\n",
            "  5. Arabic (Morocco)\n",
            "  6. Arabic (Syria)\n",
            "  7. Armenian\n",
            "  8. A user-defined custom Layout\n",
            "  9. Azerbaijani\n",
            "  10. Bambara\n",
            "  11. Bangla\n",
            "  12. Belarusian\n",
            "  13. Belgian\n",
            "  14. Berber (Algeria, Latin)\n",
            "  15. Bosnian\n",
            "  16. Braille\n",
            "  17. Bulgarian\n",
            "  18. Burmese\n",
            "  19. Chinese\n",
            "  20. Croatian\n",
            "  21. Czech\n",
            "  22. Danish\n",
            "  23. Dhivehi\n",
            "  24. Dutch\n",
            "  25. Dzongkha\n",
            "  26. English (Australian)\n",
            "  27. English (Cameroon)\n",
            "  28. English (Ghana)\n",
            "  29. English (Nigeria)\n",
            "  30. English (South Africa)\n",
            "  31. English (UK)\n",
            "  32. English (US)\n",
            "  33. Esperanto\n",
            "  34. Estonian\n",
            "  35. Faroese\n",
            "  36. Filipino\n",
            "  37. Finnish\n",
            "  38. French\n",
            "  39. French (Canada)\n",
            "  40. French (Democratic Republic of the Congo)\n",
            "  41. French (Togo)\n",
            "  42. Georgian\n",
            "  43. German\n",
            "  44. German (Austria)\n",
            "  45. Greek\n",
            "  46. Hebrew\n",
            "  47. Hungarian\n",
            "  48. Icelandic\n",
            "  49. Indian\n",
            "  50. Indonesian (Javanese)\n",
            "  51. Indonesian (Latin)\n",
            "  52. Iraqi\n",
            "  53. Irish\n",
            "  54. Italian\n",
            "  55. Japanese\n",
            "  56. Japanese (PC-98)\n",
            "  57. Kazakh\n",
            "  58. Khmer (Cambodia)\n",
            "  59. Korean\n",
            "  60. Kyrgyz\n",
            "  61. Lao\n",
            "  62. Latvian\n",
            "  63. Lithuanian\n",
            "  64. Macedonian\n",
            "  65. Malay (Jawi, Arabic Keyboard)\n",
            "  66. Maltese\n",
            "  67. Maori\n",
            "  68. Moldavian\n",
            "  69. Mongolian\n",
            "  70. Montenegrin\n",
            "  71. Nepali\n",
            "  72. NKo (AZERTY)\n",
            "  73. Norwegian\n",
            "  74. Persian\n",
            "  75. Polish\n",
            "  76. Portuguese\n",
            "  77. Portuguese (Brazil)\n",
            "  78. Romanian\n",
            "  79. Russian\n",
            "  80. Serbian\n",
            "  81. Sinhala (phonetic)\n",
            "  82. Slovak\n",
            "  83. Slovenian\n",
            "  84. Spanish\n",
            "  85. Spanish (Latin American)\n",
            "  86. Swahili (Kenya)\n",
            "  87. Swahili (Tanzania)\n",
            "  88. Swedish\n",
            "  89. Switzerland\n",
            "  90. Taiwanese\n",
            "  91. Tajik\n",
            "  92. Thai\n",
            "  93. Tswana\n",
            "  94. Turkish\n",
            "  95. Turkmen\n",
            "  96. Ukrainian\n",
            "  97. Urdu (Pakistan)\n",
            "  98. Uzbek\n",
            "  99. Vietnamese\n",
            "  100. Wolof\n",
            "\u001b[4mCountry of origin for the keyboard: \u001b[m\u001b[1m32\n",
            "\u001b[m\u001b[m\n",
            "Please select the layout matching the keyboard for this machine.\n",
            "\n",
            "  1. English (US)\n",
            "  2. English (US) - Cherokee\n",
            "  3. English (US) - English (classic Dvorak)\n",
            "  4. English (US) - English (Colemak)\n",
            "  5. English (US) - English (Colemak-DH)\n",
            "  6. English (US) - English (Colemak-DH ISO)\n",
            "  7. English (US) - English (Dvorak)\n",
            "  8. English (US) - English (Dvorak, alt. intl.)\n",
            "  9. English (US) - English (Dvorak, intl., with dead keys)\n",
            "  10. English (US) - English (Dvorak, left-handed)\n",
            "  11. English (US) - English (Dvorak, right-handed)\n",
            "  12. English (US) - English (intl., with AltGr dead keys)\n",
            "  13. English (US) - English (Macintosh)\n",
            "  14. English (US) - English (Norman)\n",
            "  15. English (US) - English (programmer Dvorak)\n",
            "  16. English (US) - English (the divide/multiply toggle the layout)\n",
            "  17. English (US) - English (US, alt. intl.)\n",
            "  18. English (US) - English (US, euro on 5)\n",
            "  19. English (US) - English (US, intl., with dead keys)\n",
            "  20. English (US) - English (US, Symbolic)\n",
            "  21. English (US) - English (Workman)\n",
            "  22. English (US) - English (Workman, intl., with dead keys)\n",
            "  23. English (US) - Hawaiian\n",
            "  24. English (US) - Russian (US, phonetic)\n",
            "  25. English (US) - Serbo-Croatian (US)\n",
            "\u001b[4mKeyboard layout: \u001b[m\u001b[1m1\n",
            "\u001b[m\u001b[m\n",
            "Your console font configuration will be updated the next time your system\n",
            "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
            "Setting up cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-encode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-samples-11-3 (11.3.58-1) ...\n",
            "Setting up xserver-xorg-video-nvidia-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Setting up dkms (2.8.7-2ubuntu2.2) ...\n",
            "Setting up nvidia-dkms-555 (555.42.06-0ubuntu1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Loading new nvidia-555.42.06 DKMS files...\n",
            "It is likely that 6.1.85+ belongs to a chroot's host\n",
            "Building for 5.15.0-117-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 5.15.0-117-generic\n",
            "Done.\n",
            "\n",
            "nvidia.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-peermem.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "Setting up cuda-tools-11-3 (11.3.0-1) ...\n",
            "Setting up nvidia-driver-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Setting alternatives\n",
            "update-alternatives: using /usr/local/cuda-11.3 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
            "Setting up cuda-drivers-555 (555.42.06-1) ...\n",
            "Setting up cuda-drivers (555.42.06-1) ...\n",
            "Setting up cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-11-3 (11.3.0-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUDA_PATH=/usr/local/cuda-11.3/\n",
        "#!export PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/"
      ],
      "metadata": {
        "id": "PuthK8ce16jt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claude says that the !export doesn't persist outside of the cell. Whoops."
      ],
      "metadata": {
        "id": "qzBJtnmg9CcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek_OMQGz8aeR",
        "outputId": "7a4ec44e-15d1-4cb9-8fdf-f4daeaf24323"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.3'\n",
        "os.environ['PATH'] = '/usr/local/cuda-11.3/bin:' + os.environ['PATH']\n",
        "\n",
        "# THIS WORKED! YAY!!"
      ],
      "metadata": {
        "id": "cBSdi8Nt9FoS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we install a GCC version compatible:"
      ],
      "metadata": {
        "id": "PorxhJIg-1q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential software-properties-common -y\n",
        "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc-9 g++-9 -y\n",
        "!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9\n",
        "!gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaBumXpw-1TR",
        "outputId": "643c39f6-bc3e-46b2-9e9b-b351734bf8c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r            \rIgn:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r                                                                               \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r                                                                               \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [2 Release 0 B/564 B 0%] [Connecting to archive.ubuntu.com (91.189.91.83)] [\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connectin\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\r                                                                               \rIgn:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
            "PPA publishes dbgsym, you may need to include 'main/debug' component\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu/ jammy main'\n",
            "Description:\n",
            "Toolchain test builds; see https://wiki.ubuntu.com/ToolChain\n",
            "\n",
            "More info: https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test\n",
            "Adding repository.\n",
            "Found existing deb entry in /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding deb entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Found existing deb-src entry in /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/ubuntu-toolchain-r-ubuntu-test.gpg with fingerprint 60C317803A41BA51845E371A1E9377A2BA9EF27F\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++-9 is already the newest version (9.5.0-1ubuntu1~22.04).\n",
            "gcc-9 is already the newest version (9.5.0-1ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works, but you will have to deal with this girthy installation every time you want to use this notebook."
      ],
      "metadata": {
        "id": "VQUnsKUL9SMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEQw31bw2efT",
        "outputId": "cb2e59aa-ce9d-4f30-c7d4-b7c8cc2b46b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /usr/local/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7ZeXia826Me",
        "outputId": "9b1e5831-e4bf-4492-fdcf-909e7ed55f78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LCkzkE0V2-jj",
        "outputId": "8355d6f0-5d07-40bd-bc47-adb09539bf6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL_i-rGN3ApR",
        "outputId": "3e392257-38cc-4116-dc0d-879ea0e140f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/    \u001b[01;36mcuda\u001b[0m@     \u001b[01;34mcuda-11.3\u001b[0m/  \u001b[01;34mcuda-12.2\u001b[0m/  \u001b[01;34mgames\u001b[0m/               \u001b[01;34minclude\u001b[0m/  \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mopt\u001b[0m/   \u001b[01;34mshare\u001b[0m/\n",
            "\u001b[01;34mcolab\u001b[0m/  \u001b[01;36mcuda-11\u001b[0m@  \u001b[01;36mcuda-12\u001b[0m@    \u001b[01;34metc\u001b[0m/        \u001b[01;34m_gcs_config_ops.so\u001b[0m/  \u001b[01;34mlib\u001b[0m/      \u001b[01;36mman\u001b[0m@    \u001b[01;34msbin\u001b[0m/  \u001b[01;34msrc\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di2B0ntq3Dfo",
        "outputId": "0de37d84-fb34-45d0-8693-29524417fea0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsL6yl_dq3PO",
        "outputId": "4a7d760b-ed81-472d-e3ab-703b607a1c7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9o_FSLH9ctO",
        "outputId": "92c719ae-0dbb-4bef-96c0-56f686048072"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/mit-han-lab/smoothquant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muFQ_9Zgfk2L",
        "outputId": "a0a03911-3fdc-4e4f-921e-54fb32bc0ac7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'smoothquant'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 352 (delta 130), reused 117 (delta 101), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (352/352), 6.80 MiB | 25.98 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd smoothquant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jsoZyglf5x0",
        "outputId": "8675ea37-1971-4452-a89e-44c31bebc030"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/smoothquant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0teKFDFGf_lT",
        "outputId": "56b52b25-4069-47a9-be5c-d4f8b36ac670"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/smoothquant\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: smoothquant\n",
            "  Attempting uninstall: smoothquant\n",
            "    Found existing installation: smoothquant 0.0.0\n",
            "    Uninstalling smoothquant-0.0.0:\n",
            "      Successfully uninstalled smoothquant-0.0.0\n",
            "  Running setup.py develop for smoothquant\n",
            "Successfully installed smoothquant-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1188LC3LtZYt",
        "outputId": "fbe8b0c8-7d36-4b1b-d141-46dac43f540a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Guangxuan-Xiao/torch-int.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4AytQdgi_gj",
        "outputId": "651ab4dc-8469-462a-dc50-4ab2004ce2aa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-int'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1102 (delta 139), reused 118 (delta 118), pack-reused 929\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 960.91 KiB | 5.89 MiB/s, done.\n",
            "Resolving deltas: 100% (643/643), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd torch-int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjJqyK7jwSP",
        "outputId": "4cfae4bb-0504-451d-e940-7d340a5167fb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also had to jump through some hoops trying to figure out this submodule stuff—it is not exactly what the repo says to do"
      ],
      "metadata": {
        "id": "FbQYofhC94cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git config submodule.submodules/cutlass.url https://github.com/NVIDIA/cutlass.git\n",
        "! git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI103aFzp_RG",
        "outputId": "d64ef4c8-4b3e-42b0-e464-08c6eb3e06d5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/torch-int/submodules/cutlass'...\n",
            "Submodule path 'submodules/cutlass': checked out 'c975e2ccbb2dbf13024568b37ffa3498ed0b3aed'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display cuda\n",
        "\n",
        "!sudo update-alternatives --config cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2k5e63-Uwg",
        "outputId": "7db87391-c655-4c12-9e5d-05f34a4b6898"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda - auto mode\n",
            "  link best version is /usr/local/cuda-12.2\n",
            "  link currently points to /usr/local/cuda-12.2\n",
            "  link cuda is /usr/local/cuda\n",
            "/usr/local/cuda-11.3 - priority 113\n",
            "/usr/local/cuda-12.2 - priority 122\n",
            "There are 2 choices for the alternative cuda (providing /usr/local/cuda).\n",
            "\n",
            "  Selection    Path                  Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/local/cuda-12.2   122       auto mode\n",
            "  1            /usr/local/cuda-11.3   113       manual mode\n",
            "  2            /usr/local/cuda-12.2   122       manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/local/cuda-11.3 to provide /usr/local/cuda (cuda) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2R7WxtNCBYZ",
        "outputId": "c35d570e-6e2d-4c43-ba46-3ae4c1209491"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yet, when I try to build_cutlass.sh, the CUDA compiler is still identified as 12.2.140. The way to fix this is the cell above, changing the alternatives for CUDA usage"
      ],
      "metadata": {
        "id": "oREC2Ch-9xRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display gcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1zmiVBCElD",
        "outputId": "bee56dab-fed9-4c5d-a013-e097f253b568"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc - auto mode\n",
            "  link best version is /usr/bin/gcc-9\n",
            "  link currently points to /usr/bin/gcc-9\n",
            "  link gcc is /usr/bin/gcc\n",
            "  slave g++ is /usr/bin/g++\n",
            "/usr/bin/gcc-9 - priority 90\n",
            "  slave g++: /usr/bin/g++-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! export CC=/path/to/gcc-9.5\n",
        "! export CXX=/path/to/g++-9.5"
      ],
      "metadata": {
        "id": "5U7wdbPaClhA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! set(CMAKE_C_COMPILER /path/to/gcc-9.5)\n",
        "! set(CMAKE_CXX_COMPILER /path/to/g++-9.5)"
      ],
      "metadata": {
        "id": "4cnBSdrHD2Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /usr/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeyiR3EfCv7V",
        "outputId": "236ac302-3150-4820-ccb1-50588d67421e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This still gave me errors for a bit, because you need to use a GCC compatible with the different CUDA version. But even when I installed a new GCC version, it seemed to use it but didn't? Weird."
      ],
      "metadata": {
        "id": "LM_FmyANCmtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! source environment.sh\n",
        "! bash build_cutlass.sh   # this one needs GPU\n",
        "! python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DSE8hTjCIU",
        "outputId": "3908a97f-3b00-4ab2-b532-8bb7fc8d91d1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- CMake Version: 3.30.1\n",
            "-- The CXX compiler identification is GNU 9.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 11.3.58\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- CUDART: /usr/local/cuda/lib64/libcudart.so\n",
            "-- CUDA Driver: /usr/local/cuda/lib64/stubs/libcuda.so\n",
            "-- NVRTC: /usr/local/cuda/lib64/libnvrtc.so\n",
            "-- Default Install Location: install\n",
            "-- CUDA Compilation Architectures: 80\n",
            "-- Enable caching of reference results in conv unit tests\n",
            "-- Enable rigorous conv problem sizes in conv unit tests\n",
            "-- Using NVCC flags: -DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;$<$<BOOL:1>:-Xcompiler=-Wconversion>;$<$<BOOL:1>:-Xcompiler=-fno-strict-aliasing>\n",
            "-- CUTLASS Revision: c975e2cc\n",
            "-- Configuring cublas ...\n",
            "-- cuBLAS Disabled.\n",
            "-- Configuring cuBLAS ... done.\n",
            "-- Found Python3: /usr/local/bin/python (found suitable version \"3.10.12\", minimum required is \"3.5\") found components: Interpreter\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c09b164b76dc.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.1134c512275c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.70bca42e032c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7412477fb00b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7cb784ae67c3.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.452e17cedfde.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.21cd46163fea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.f2656f9851f5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.da510d6f1a23.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed2bb5981d55.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.22f9e56a890e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.17417f539ca2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.d1a5010bad63.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5a217a522f90.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e764a5c32d2e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.00a490a48b54.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.67748a1f69fd.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.aaae24350764.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a7556fb0c363.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.cdd7679105ea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5b5abd9da9bb.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0188c1e5012f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c1136defa56d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.4efc263febd5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b1c066c89ad1.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.059188bf387f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dc6fdf4e3631.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed319cdaa486.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7f69d119038b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ab4c8725d89a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.db0920d13551.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.53863dce2b34.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5206989612aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a40fdb431023.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.999493302618.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.bbd766ae1258.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.3aca44f87dc7.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.fe5af8d45b59.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2d9ae5d87c7a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8112247fe822.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.447ac2531506.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b7cdd3426b6c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b83304556123.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c2bae868a773.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.51d0f833ea6d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dd6b23b6f933.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.eac06d5a86ae.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ec9d28d7c560.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.25dee0d0208a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.41f602329ae6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.9763c0db7135.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2fdf36a298c6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0a8164c28c5f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a4cfc29c733f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8a2ca8aaf7aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.977a56f2bc38.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e837fe821ee2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.6f06994db371.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ae96169b4e45.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a6a0ceb84086.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.19976851ec6d.cu\n",
            "-- Configuring done (3.2s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/torch-int/submodules/cutlass/build\n",
            "[  0%] \u001b[32mBuilding CUDA object examples/00_basic_gemm/CMakeFiles/00_basic_gemm.dir/basic_gemm.cu.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CUDA object examples/01_cutlass_utilities/CMakeFiles/01_cutlass_utilities.dir/cutlass_utilities.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/02_dump_reg_shmem/CMakeFiles/02_dump_reg_shmem.dir/dump_reg_shmem.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/visualize_layout.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/04_tile_iterator/CMakeFiles/04_tile_iterator.dir/tile_iterator.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/register_layout.cu.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CUDA object examples/05_batched_gemm/CMakeFiles/05_batched_gemm.dir/batched_gemm.cu.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object examples/07_volta_tensorop_gemm/CMakeFiles/07_volta_tensorop_gemm.dir/volta_tensorop_gemm.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object examples/08_turing_tensorop_gemm/CMakeFiles/08_turing_tensorop_gemm.dir/turing_tensorop_gemm.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object examples/06_splitK_gemm/CMakeFiles/06_splitK_gemm.dir/splitk_gemm.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/handle.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/src/manifest.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/09_turing_tensorop_conv2dfprop/CMakeFiles/09_turing_tensorop_conv2dfprop.dir/turing_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_rf.dir/fused_two_convs_f16_sm75_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/12_gemm_bias_relu/CMakeFiles/12_gemm_bias_relu.dir/gemm_bias_relu.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_shmem.dir/fused_two_convs_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/operation_table.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/singleton.cu.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CUDA executable 04_tile_iterator\u001b[0m\n",
            "[  9%] Built target 04_tile_iterator\n",
            "[  9%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_rf.dir/fused_two_convs_f16_sm80_rf.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/util.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/gemm.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CUDA executable 02_dump_reg_shmem\u001b[0m\n",
            "[ 10%] Built target 02_dump_reg_shmem\n",
            "[ 11%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_shmem.dir/fused_two_convs_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/initialize_reference_operations.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/reduction_device.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32m\u001b[1mLinking CUDA executable 00_basic_gemm\u001b[0m\n",
            "[ 12%] Built target 00_basic_gemm\n",
            "[ 13%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_rf.dir/fused_two_convs_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/init_reduction_operations.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CUDA executable 08_turing_tensorop_gemm\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX executable 03_visualize_layout\u001b[0m\n",
            "[ 15%] Built target 03_visualize_layout\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CUDA executable 12_gemm_bias_relu\u001b[0m\n",
            "[ 16%] Built target 08_turing_tensorop_gemm\n",
            "[ 17%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_shmem.dir/fused_two_convs_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_rf.dir/fused_two_convs_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 18%] Built target 12_gemm_bias_relu\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_shmem.dir/fused_two_convs_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 01_cutlass_utilities\u001b[0m\n",
            "[ 19%] Built target 01_cutlass_utilities\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_rf.dir/fused_two_gemms_f16_sm75_rf.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 09_turing_tensorop_conv2dfprop\u001b[0m\n",
            "[ 19%] Built target 09_turing_tensorop_conv2dfprop\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_shmem.dir/fused_two_gemms_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_rf.dir/fused_two_gemms_f16_sm80_rf.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 05_batched_gemm\u001b[0m\n",
            "[ 19%] Built target 05_batched_gemm\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_shmem.dir/fused_two_gemms_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv2d.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_rf\u001b[0m\n",
            "[ 20%] Built target 13_fused_two_convs_f16_sm75_rf\n",
            "[ 20%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_rf.dir/fused_two_gemms_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_shmem\u001b[0m\n",
            "[ 21%] Built target 13_fused_two_convs_f16_sm75_shmem\n",
            "[ 22%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_shmem.dir/fused_two_gemms_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_rf\u001b[0m\n",
            "[ 22%] Built target 13_fused_two_convs_f16_sm80_rf\n",
            "[ 23%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_rf.dir/fused_two_gemms_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_shmem\u001b[0m\n",
            "[ 23%] Built target 13_fused_two_convs_f16_sm80_shmem\n",
            "[ 23%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv3d.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_rf\u001b[0m\n",
            "[ 24%] Built target 13_fused_two_gemms_f16_sm75_rf\n",
            "[ 25%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/generated/initialize_all.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c09b164b76dc.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_shmem\u001b[0m\n",
            "[ 26%] Built target 13_fused_two_gemms_f16_sm75_shmem\n",
            "[ 27%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_shmem.dir/fused_two_gemms_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_rf\u001b[0m\n",
            "[ 28%] Built target 13_fused_two_gemms_f16_sm80_rf\n",
            "[ 29%] \u001b[32mBuilding CUDA object examples/14_ampere_tf32_tensorop_gemm/CMakeFiles/14_ampere_tf32_tensorop_gemm.dir/ampere_tf32_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_rf\u001b[0m\n",
            "[ 29%] Built target 13_fused_two_convs_s8_sm75_rf\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/15_ampere_sparse_tensorop_gemm/CMakeFiles/15_ampere_sparse_tensorop_gemm.dir/ampere_sparse_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_shmem\u001b[0m\n",
            "[ 31%] Built target 13_fused_two_gemms_f16_sm80_shmem\n",
            "[ 31%] \u001b[32mBuilding CUDA object examples/16_ampere_tensorop_conv2dfprop/CMakeFiles/16_ampere_tensorop_conv2dfprop.dir/ampere_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_shmem\u001b[0m\n",
            "[ 31%] Built target 13_fused_two_convs_s8_sm75_shmem\n",
            "[ 31%] \u001b[32mBuilding CUDA object examples/17_fprop_per_channel_bias/CMakeFiles/17_fprop_per_channel_bias.dir/fprop_per_channel_bias.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_rf\u001b[0m\n",
            "[ 31%] Built target 13_fused_two_convs_s8_sm80_rf\n",
            "[ 32%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.1134c512275c.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_shmem\u001b[0m\n",
            "[ 32%] Built target 13_fused_two_convs_s8_sm80_shmem\n",
            "[ 32%] \u001b[32mBuilding CUDA object examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeFiles/18_ampere_fp64_tensorop_affine2_gemm.dir/ampere_fp64_tensorop_affine2_gemm.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_rf\u001b[0m\n",
            "[ 32%] Built target 13_fused_two_gemms_s8_sm75_rf\n",
            "[ 32%] \u001b[32mBuilding CUDA object examples/19_tensorop_canonical/CMakeFiles/19_tensorop_canonical.dir/tensorop_canonical.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_shmem\u001b[0m\n",
            "[ 32%] Built target 13_fused_two_gemms_s8_sm75_shmem\n",
            "[ 32%] \u001b[32mBuilding CUDA object examples/20_simt_canonical/CMakeFiles/20_simt_canonical.dir/simt_canonical.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 19_tensorop_canonical\u001b[0m\n",
            "[ 33%] Built target 19_tensorop_canonical\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/21_quaternion_gemm/CMakeFiles/21_quaternion_gemm.dir/quaternion_gemm.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 14_ampere_tf32_tensorop_gemm\u001b[0m\n",
            "[ 33%] Built target 14_ampere_tf32_tensorop_gemm\n",
            "[ 33%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.70bca42e032c.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_rf\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 15_ampere_sparse_tensorop_gemm\u001b[0m\n",
            "[ 33%] Built target 13_fused_two_gemms_s8_sm80_rf\n",
            "[ 33%] Built target 15_ampere_sparse_tensorop_gemm\n",
            "[ 34%] \u001b[32mBuilding CUDA object examples/22_quaternion_conv/CMakeFiles/22_quaternion_conv.dir/quaternion_conv.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object examples/23_ampere_gemm_operand_reduction_fusion/CMakeFiles/23_ampere_gemm_operand_reduction_fusion.dir/ampere_gemm_operand_reduction_fusion.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA executable 17_fprop_per_channel_bias\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CUDA executable 20_simt_canonical\u001b[0m\n",
            "[ 37%] Built target 17_fprop_per_channel_bias\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CUDA executable 18_ampere_fp64_tensorop_affine2_gemm\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CUDA object examples/24_gemm_grouped/CMakeFiles/24_gemm_grouped.dir/gemm_grouped.cu.o\u001b[0m\n",
            "[ 39%] Built target 20_simt_canonical\n",
            "[ 40%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_fprop_mainloop_fusion.dir/ampere_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 40%] Built target 18_ampere_fp64_tensorop_affine2_gemm\n",
            "[ 41%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_3d_fprop_mainloop_fusion.dir/ampere_3d_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 16_ampere_tensorop_conv2dfprop\u001b[0m\n",
            "[ 42%] Built target 16_ampere_tensorop_conv2dfprop\n",
            "[ 42%] \u001b[32mBuilding CUDA object examples/26_ampere_wgrad_mainloop_fusion/CMakeFiles/26_ampere_wgrad_mainloop_fusion.dir/ampere_wgrad_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7412477fb00b.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_shmem\u001b[0m\n",
            "[ 43%] Built target 13_fused_two_gemms_s8_sm80_shmem\n",
            "[ 43%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7cb784ae67c3.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CUDA executable 21_quaternion_gemm\u001b[0m\n",
            "[ 43%] Built target 21_quaternion_gemm\n",
            "[ 43%] \u001b[32mBuilding CUDA object examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeFiles/27_ampere_3xtf32_fast_accurate_tensorop_gemm.dir/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CUDA executable 23_ampere_gemm_operand_reduction_fusion\u001b[0m\n",
            "[ 43%] Built target 23_ampere_gemm_operand_reduction_fusion\n",
            "[ 44%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.452e17cedfde.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_3d_fprop_mainloop_fusion\u001b[0m\n",
            "[ 44%] Built target 25_ampere_3d_fprop_mainloop_fusion\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeFiles/28_ampere_3xtf32_fast_accurate_tensorop_fprop.dir/ampere_3xtf32_fast_accurate_tensorop_fprop.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 22_quaternion_conv\u001b[0m\n",
            "[ 44%] Built target 22_quaternion_conv\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeFiles/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.dir/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_fprop_mainloop_fusion\u001b[0m\n",
            "[ 44%] Built target 25_ampere_fprop_mainloop_fusion\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/30_wgrad_split_k/CMakeFiles/30_wgrad_split_k.dir/30_wgrad_split_k.cu.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CUDA executable 26_ampere_wgrad_mainloop_fusion\u001b[0m\n",
            "[ 45%] Built target 26_ampere_wgrad_mainloop_fusion\n",
            "[ 45%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.21cd46163fea.cu.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CUDA executable 27_ampere_3xtf32_fast_accurate_tensorop_gemm\u001b[0m\n",
            "[ 46%] Built target 27_ampere_3xtf32_fast_accurate_tensorop_gemm\n",
            "[ 47%] \u001b[32mBuilding CUDA object examples/31_basic_syrk/CMakeFiles/31_basic_syrk.dir/basic_syrk.cu.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.f2656f9851f5.cu.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CUDA executable 24_gemm_grouped\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CUDA executable 06_splitK_gemm\u001b[0m\n",
            "[ 48%] Built target 24_gemm_grouped\n",
            "[ 49%] \u001b[32mBuilding CUDA object examples/32_basic_trmm/CMakeFiles/32_basic_trmm.dir/basic_trmm.cu.o\u001b[0m\n",
            "[ 49%] Built target 06_splitK_gemm\n",
            "[ 50%] \u001b[32mBuilding CUDA object examples/33_ampere_3xtf32_tensorop_symm/CMakeFiles/33_ampere_3xtf32_tensorop_symm.dir/ampere_3xtf32_tensorop_symm.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable 30_wgrad_split_k\u001b[0m\n",
            "[ 50%] Built target 30_wgrad_split_k\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/34_transposed_conv2d/CMakeFiles/34_transposed_conv2d.dir/34_transposed_conv2d.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CUDA executable 28_ampere_3xtf32_fast_accurate_tensorop_fprop\u001b[0m\n",
            "[ 52%] Built target 28_ampere_3xtf32_fast_accurate_tensorop_fprop\n",
            "[ 52%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.da510d6f1a23.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CUDA executable 31_basic_syrk\u001b[0m\n",
            "[ 52%] Built target 31_basic_syrk\n",
            "[ 52%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed2bb5981d55.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CUDA executable 07_volta_tensorop_gemm\u001b[0m\n",
            "[ 52%] Built target 07_volta_tensorop_gemm\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/35_gemm_softmax/CMakeFiles/35_gemm_softmax.dir/gemm_softmax.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 32_basic_trmm\u001b[0m\n",
            "[ 53%] Built target 32_basic_trmm\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/36_gather_scatter_fusion/CMakeFiles/36_gather_scatter_fusion.dir/gather_scatter_fusion.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 34_transposed_conv2d\u001b[0m\n",
            "[ 53%] Built target 34_transposed_conv2d\n",
            "[ 54%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.22f9e56a890e.cu.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\u001b[0m\n",
            "[ 55%] Built target 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\n",
            "[ 55%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.17417f539ca2.cu.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable 33_ampere_3xtf32_tensorop_symm\u001b[0m\n",
            "[ 55%] Built target 33_ampere_3xtf32_tensorop_symm\n",
            "[ 56%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.d1a5010bad63.cu.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5a217a522f90.cu.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CUDA executable 36_gather_scatter_fusion\u001b[0m\n",
            "[ 57%] Built target 36_gather_scatter_fusion\n",
            "[ 57%] \u001b[32mBuilding CUDA object examples/37_gemm_layernorm_gemm_fusion/CMakeFiles/37_gemm_layernorm_gemm_fusion.dir/gemm_layernorm.cu.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e764a5c32d2e.cu.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.00a490a48b54.cu.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CUDA executable 35_gemm_softmax\u001b[0m\n",
            "[ 58%] Built target 35_gemm_softmax\n",
            "[ 58%] \u001b[32mBuilding CUDA object examples/38_syr2k_grouped/CMakeFiles/38_syr2k_grouped.dir/syr2k_grouped.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(507): warning: variable \"minus\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(508): warning: variable \"mul\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(509): warning: variable \"exponential\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "[ 58%] \u001b[32mBuilding CUDA object examples/39_gemm_permute/CMakeFiles/39_gemm_permute.dir/gemm_permute.cu.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_fixed_seqlen.dir/fused_multihead_attention_fixed_seqlen.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1083): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "[ 59%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.67748a1f69fd.cu.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CUDA executable 37_gemm_layernorm_gemm_fusion\u001b[0m\n",
            "[ 60%] Built target 37_gemm_layernorm_gemm_fusion\n",
            "[ 60%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_variable_seqlen.dir/fused_multihead_attention_variable_seqlen.cu.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.aaae24350764.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "[ 61%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a7556fb0c363.cu.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CUDA executable 39_gemm_permute\u001b[0m\n",
            "[ 62%] Built target 39_gemm_permute\n",
            "[ 62%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.cdd7679105ea.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CUDA executable 38_syr2k_grouped\u001b[0m\n",
            "[ 63%] Built target 38_syr2k_grouped\n",
            "[ 63%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5b5abd9da9bb.cu.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0188c1e5012f.cu.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c1136defa56d.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.4efc263febd5.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_fixed_seqlen\u001b[0m\n",
            "[ 66%] Built target 41_fused_multi_head_attention_fixed_seqlen\n",
            "[ 66%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b1c066c89ad1.cu.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.059188bf387f.cu.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dc6fdf4e3631.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed319cdaa486.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7f69d119038b.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ab4c8725d89a.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_variable_seqlen\u001b[0m\n",
            "[ 69%] Built target 41_fused_multi_head_attention_variable_seqlen\n",
            "[ 70%] \u001b[32mBuilding CUDA object examples/42_ampere_tensorop_group_conv/CMakeFiles/42_ampere_tensorop_group_conv.dir/ampere_tensorop_group_conv.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.db0920d13551.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.53863dce2b34.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5206989612aa.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a40fdb431023.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object examples/43_ell_block_sparse_gemm/CMakeFiles/43_ell_block_sparse_gemm.dir/ell_block_sparse_gemm.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object examples/45_dual_gemm/CMakeFiles/45_dual_gemm.dir/dual_gemm.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CUDA executable 42_ampere_tensorop_group_conv\u001b[0m\n",
            "[ 73%] Built target 42_ampere_tensorop_group_conv\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.999493302618.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.bbd766ae1258.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.3aca44f87dc7.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object examples/46_depthwise_simt_conv2dfprop/CMakeFiles/46_depthwise_simt_conv2dfprop.dir/depthwise_simt_conv2dfprop.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CUDA executable 43_ell_block_sparse_gemm\u001b[0m\n",
            "[ 76%] Built target 43_ell_block_sparse_gemm\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.fe5af8d45b59.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CUDA executable 45_dual_gemm\u001b[0m\n",
            "[ 76%] Built target 45_dual_gemm\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2d9ae5d87c7a.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8112247fe822.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.447ac2531506.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CUDA executable 46_depthwise_simt_conv2dfprop\u001b[0m\n",
            "[ 77%] Built target 46_depthwise_simt_conv2dfprop\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b7cdd3426b6c.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b83304556123.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c2bae868a773.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.51d0f833ea6d.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dd6b23b6f933.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.eac06d5a86ae.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ec9d28d7c560.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.25dee0d0208a.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.41f602329ae6.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.9763c0db7135.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2fdf36a298c6.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0a8164c28c5f.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a4cfc29c733f.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8a2ca8aaf7aa.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.977a56f2bc38.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e837fe821ee2.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.6f06994db371.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ae96169b4e45.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a6a0ceb84086.cu.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.19976851ec6d.cu.o\u001b[0m\n",
            "[ 87%] Built target cutlass_library_objs\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX shared library libcutlass.so\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX static library libcutlass.a\u001b[0m\n",
            "[ 88%] Built target cutlass_lib\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/10_planar_complex/CMakeFiles/10_planar_complex.dir/planar_complex.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/11_planar_complex_array/CMakeFiles/11_planar_complex_array.dir/planar_complex_array.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/options.cu.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cutlass_profiler.cu.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/performance_report.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/enumerated_types.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_allocation.cu.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gpu_timer.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_context.cu.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cublas_helpers.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cudnn_helpers.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/problem_space.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_k_operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] Built target cutlass_library_static\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_2k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/trmm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/symm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv2d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv3d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/sparse_gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CUDA executable 11_planar_complex_array\u001b[0m\n",
            "[ 98%] Built target 11_planar_complex_array\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CUDA executable 10_planar_complex\u001b[0m\n",
            "[ 99%] Built target 10_planar_complex\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable cutlass_profiler\u001b[0m\n",
            "[100%] Built target cutlass_profiler\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing torch_int.egg-info/PKG-INFO\n",
            "writing dependency_links to torch_int.egg-info/dependency_links.txt\n",
            "writing top-level names to torch_int.egg-info/top_level.txt\n",
            "reading manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying torch_int/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int\n",
            "copying torch_int/models/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/models/opt.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/nn/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/linear.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/quantization.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/utils\n",
            "running build_ext\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/torch-int/setup.py\", line 9, in <module>\n",
            "    setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 108, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install.py\", line 87, in run\n",
            "    self.do_egg_install()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install.py\", line 139, in do_egg_install\n",
            "    self.run_command('bdist_egg')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 167, in run\n",
            "    cmd = self.call_command('install_lib', warn_dir=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/bdist_egg.py\", line 153, in call_command\n",
            "    self.run_command(cmdname)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/install_lib.py\", line 12, in run\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/install_lib.py\", line 110, in build\n",
            "    self.run_command('build_ext')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "    self.distribution.run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 956, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/build_ext.py\", line 93, in run\n",
            "    _build_ext.run(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "    self.build_extensions()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 434, in build_extensions\n",
            "    self._check_cuda_version(compiler_name, compiler_version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 836, in _check_cuda_version\n",
            "    raise RuntimeError(\n",
            "RuntimeError: The current installed version of x86_64-linux-gnu-g++ (11.4.0) is greater than the maximum required version by CUDA 11.3 (10.0.0). Please make sure to use an adequate version of x86_64-linux-gnu-g++ (>=5.0.0, <=10.0.0).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "id": "7kZ5MNKBv_ko",
        "outputId": "e0a256ca-e943-46a8-d0ab-906a538b5f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import smoothquant"
      ],
      "metadata": {
        "id": "FCA9Lpz6fi2u"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smoothquant.opt import Int8OPTForCausalLM\n",
        "model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "DKYIiWlbgfsq",
        "outputId": "61e6cce0-346d-43c6-8d5c-8332c45cbf17"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'smoothquant.opt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4c536754c939>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mit-han-lab/opt-30b-smoothquant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smoothquant.opt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5x-01auil_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}