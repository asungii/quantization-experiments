{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1p09ODOGXQBpkwZtKLM_QQS-Pesmqbt5t",
      "authorship_tag": "ABX9TyNbRftg0tGWQ95bsIXolQVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88ad8cafc3514d07953581defbd07b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c79c4c200af54fd78ce937da23167392",
              "IPY_MODEL_9ecac45caf224265ae5b383ac58bbf58",
              "IPY_MODEL_2233f5c653da406bbd1bde8beb5728de"
            ],
            "layout": "IPY_MODEL_e6b2d8145ea34f3f821bd210be833122"
          }
        },
        "c79c4c200af54fd78ce937da23167392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb853c5ab2a04b489585e71ac66dbeee",
            "placeholder": "​",
            "style": "IPY_MODEL_f6894f5d3d5949659f2a34074cf88872",
            "value": "pytorch_model.bin.index.json: 100%"
          }
        },
        "9ecac45caf224265ae5b383ac58bbf58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24729f0709f4d4fb53201bf6fa1d9d2",
            "max": 62801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77e58e2050154c478f36f2ba32683925",
            "value": 62801
          }
        },
        "2233f5c653da406bbd1bde8beb5728de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3865c278c443e6ab6b7f7af4bf4a4e",
            "placeholder": "​",
            "style": "IPY_MODEL_e12cc7df8d39474d9cfc2a3007ab69a2",
            "value": " 62.8k/62.8k [00:00&lt;00:00, 287kB/s]"
          }
        },
        "e6b2d8145ea34f3f821bd210be833122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb853c5ab2a04b489585e71ac66dbeee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6894f5d3d5949659f2a34074cf88872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d24729f0709f4d4fb53201bf6fa1d9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e58e2050154c478f36f2ba32683925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc3865c278c443e6ab6b7f7af4bf4a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e12cc7df8d39474d9cfc2a3007ab69a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2c2b7ce0e0a48bdad089de7049832c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd286a7920ec42ef8d1c0f603106580f",
              "IPY_MODEL_0cb5b32856694749b0a28fa9843be65a",
              "IPY_MODEL_f0c46529142d4e88a28590d113624e7a"
            ],
            "layout": "IPY_MODEL_c78d34d6d2714e1ea35a5838004efcc6"
          }
        },
        "bd286a7920ec42ef8d1c0f603106580f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ba6b593de2492eb0e0d4f1c38907f8",
            "placeholder": "​",
            "style": "IPY_MODEL_938c74e24e6e4d47913651161a36f150",
            "value": "Downloading shards:  14%"
          }
        },
        "0cb5b32856694749b0a28fa9843be65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc4b2fd0c6e4beeb0a168c4ca83e932",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d99e89d8654d54ae374ed38a687c54",
            "value": 1
          }
        },
        "f0c46529142d4e88a28590d113624e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b48d3ce182e940788f961d91aa924441",
            "placeholder": "​",
            "style": "IPY_MODEL_19b64b8d1cc84a9d95f850f8066b735a",
            "value": " 1/7 [06:52&lt;04:47, 47.98s/it]"
          }
        },
        "c78d34d6d2714e1ea35a5838004efcc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ba6b593de2492eb0e0d4f1c38907f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938c74e24e6e4d47913651161a36f150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcc4b2fd0c6e4beeb0a168c4ca83e932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d99e89d8654d54ae374ed38a687c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b48d3ce182e940788f961d91aa924441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b64b8d1cc84a9d95f850f8066b735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cdda65f05434341ad2a6e1f9c89754b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a82f25cb7f9e4455a40fc69144e2ec84",
              "IPY_MODEL_c76203f685d34ec3a89bd9e90f99d3ba",
              "IPY_MODEL_4f94d4ecd8664d6084444f32db038603"
            ],
            "layout": "IPY_MODEL_08da95c3d9b44343981165e8c9084dcb"
          }
        },
        "a82f25cb7f9e4455a40fc69144e2ec84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337773732548452aa0ae01e8dd42a74d",
            "placeholder": "​",
            "style": "IPY_MODEL_3387729198114b5794b4d616a64635b8",
            "value": "pytorch_model-00001-of-00007.bin: 100%"
          }
        },
        "c76203f685d34ec3a89bd9e90f99d3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_774a15feae0c436c8fe70dea23f4c1d4",
            "max": 9794466629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df07154c9abd4de3a72c0b385de6796d",
            "value": 9794466629
          }
        },
        "4f94d4ecd8664d6084444f32db038603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9edd189507a545e0869d876db1a58344",
            "placeholder": "​",
            "style": "IPY_MODEL_05968a0fdc644385aec700818e1408e7",
            "value": " 9.79G/9.79G [00:47&lt;00:00, 297MB/s]"
          }
        },
        "08da95c3d9b44343981165e8c9084dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337773732548452aa0ae01e8dd42a74d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3387729198114b5794b4d616a64635b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "774a15feae0c436c8fe70dea23f4c1d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df07154c9abd4de3a72c0b385de6796d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edd189507a545e0869d876db1a58344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05968a0fdc644385aec700818e1408e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "914fbe8e42864bffb27279a5642befe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a3fde74d11b4d3986251e69fea3460e",
              "IPY_MODEL_2feb72f5febb48fb893abfa9628d7c7c",
              "IPY_MODEL_7a866ccf114c439a849bcb03977d86bd"
            ],
            "layout": "IPY_MODEL_0e1ebca02ec44e018f8ca63eecb56aa6"
          }
        },
        "6a3fde74d11b4d3986251e69fea3460e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3c29cc451e4dcbb08335feab197ad4",
            "placeholder": "​",
            "style": "IPY_MODEL_c904896b10d64e7388424589e7476cd6",
            "value": "pytorch_model-00002-of-00007.bin:  39%"
          }
        },
        "2feb72f5febb48fb893abfa9628d7c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c790e6b8044e8e924c6608956a7779",
            "max": 9866534401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49ea14c4fcca468bb4a6491c2a19f69a",
            "value": 3848273920
          }
        },
        "7a866ccf114c439a849bcb03977d86bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17cff365b85246a190e2a906d0df04ea",
            "placeholder": "​",
            "style": "IPY_MODEL_7a60f653c43f4392915d7dd757391dac",
            "value": " 3.85G/9.87G [06:02&lt;06:04, 16.5MB/s]"
          }
        },
        "0e1ebca02ec44e018f8ca63eecb56aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb3c29cc451e4dcbb08335feab197ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c904896b10d64e7388424589e7476cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c790e6b8044e8e924c6608956a7779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ea14c4fcca468bb4a6491c2a19f69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17cff365b85246a190e2a906d0df04ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a60f653c43f4392915d7dd757391dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed93915d0d334917b8f13d87526640ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aad70ea63484aa5a92a5b7a0437ad19",
              "IPY_MODEL_e9ccfda2413a43ec99a6f535e178e7c7",
              "IPY_MODEL_adca1e27830c45b8854102958edcb613"
            ],
            "layout": "IPY_MODEL_efb9438c98b54307a9eff9daa0d6aa91"
          }
        },
        "8aad70ea63484aa5a92a5b7a0437ad19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe2e0a106ea44d7b4a1c027240df674",
            "placeholder": "​",
            "style": "IPY_MODEL_56f96a5b44604571818fcffd40d6483e",
            "value": "Map: 100%"
          }
        },
        "e9ccfda2413a43ec99a6f535e178e7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61df33397a6f49fab3f8d9a739f19db1",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_084b40fb59d146578d473c641880196e",
            "value": 50
          }
        },
        "adca1e27830c45b8854102958edcb613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d5602eb360a40559616369702519f42",
            "placeholder": "​",
            "style": "IPY_MODEL_15242e99713c4599b6f13955b7f40117",
            "value": " 50/50 [00:00&lt;00:00, 624.34 examples/s]"
          }
        },
        "efb9438c98b54307a9eff9daa0d6aa91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe2e0a106ea44d7b4a1c027240df674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56f96a5b44604571818fcffd40d6483e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61df33397a6f49fab3f8d9a739f19db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084b40fb59d146578d473c641880196e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d5602eb360a40559616369702519f42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15242e99713c4599b6f13955b7f40117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laerdon/quantization-experiments/blob/main/SmoothQuant_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SmoothQuant Exploration\n",
        "\n",
        "Working with SmoothQuant inference in Google Colab."
      ],
      "metadata": {
        "id": "5CBh4xbk0VHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "From SmoothQuant repo. Because the specified CUDA compiler is 11.3, and the default Colab CUDA version is as of 7/29/24 12.2, we have to change the CUDA version in Colab.\n",
        "\n",
        "I found info on how to do this from:\n",
        "\n",
        "https://medium.com/@ajithkumarv/how-to-modify-cuda-gcc-python-versions-in-colab-584ed4113157\n",
        "\n",
        "https://github.com/googlecolab/colabtools/issues/4214#issuecomment-1859155789\n",
        "\n",
        "https://vitalitylearning.medium.com/running-cuda-in-google-colab-525a92efcf75\n",
        "\n",
        "This is going to be a retry—I'm going to figure out if I can do this submodule work as intended so that this will work."
      ],
      "metadata": {
        "id": "nKLSnmrnyb2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: you can only use the base env in condacolab, so it's basically useless."
      ],
      "metadata": {
        "id": "O3gvi0ZYydWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning and installing SmoothQuant repo"
      ],
      "metadata": {
        "id": "N9RF1vcH0pPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K6NI2WxlYpUM",
        "outputId": "afda5a9a-638a-4786-e127-c05054b065d7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.10/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu113 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.7.4)\n",
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.21.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0) (2024.7.4)\n",
            "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.14.1\n",
            "    Uninstalling tokenizers-0.14.1:\n",
            "      Successfully uninstalled tokenizers-0.14.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.34.0\n",
            "    Uninstalling transformers-4.34.0:\n",
            "      Successfully uninstalled transformers-4.34.0\n",
            "Successfully installed tokenizers-0.15.2 transformers-4.36.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "85dfc621dc054bc9b37a472c358fbb59"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# setup is from the mit-han-lab GitHub\n",
        "\n",
        "# these bits are not necessary for Google Colab. conda is getting on my nerves\n",
        "# !conda create -n smoothquant python=3.8\n",
        "# !conda activate smoothquant\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers==4.36.0 accelerate datasets zstandard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/smoothquant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwpnccEtNLSP",
        "outputId": "53ada96b-b0cd-4d16-e054-fcf31af48641"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'smoothquant'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 352 (delta 130), reused 116 (delta 100), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (352/352), 6.80 MiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/smoothquant\n",
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u4grsIO0NZVg",
        "outputId": "941faa8d-9d49-4d16-d205-7ef6f3a48167"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/smoothquant\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating smoothquant.egg-info\n",
            "writing smoothquant.egg-info/PKG-INFO\n",
            "writing dependency_links to smoothquant.egg-info/dependency_links.txt\n",
            "writing top-level names to smoothquant.egg-info/top_level.txt\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "reading manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/smoothquant\n",
            "copying smoothquant/calibration.py -> build/lib/smoothquant\n",
            "copying smoothquant/__init__.py -> build/lib/smoothquant\n",
            "copying smoothquant/fake_quant.py -> build/lib/smoothquant\n",
            "copying smoothquant/opt.py -> build/lib/smoothquant\n",
            "copying smoothquant/ppl_eval.py -> build/lib/smoothquant\n",
            "copying smoothquant/smooth.py -> build/lib/smoothquant\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/calibration.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/__init__.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/fake_quant.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/opt.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/ppl_eval.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/smooth.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/calibration.py to calibration.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/fake_quant.py to fake_quant.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/opt.py to opt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/ppl_eval.py to ppl_eval.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/smooth.py to smooth.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/smoothquant-0.0.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing smoothquant-0.0.0-py3.10.egg\n",
            "Copying smoothquant-0.0.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding smoothquant 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg\n",
            "Processing dependencies for smoothquant==0.0.0\n",
            "Finished processing dependencies for smoothquant==0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing CUDA version from 12.2 --> 11.3"
      ],
      "metadata": {
        "id": "yfB1Pp_90_H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "\n",
        "# well it's 12.2. that's a bit of an issue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azoig1mPz7bV",
        "outputId": "8fe2640b-2920-46af-f49c-be68ec4d5fc3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
        "!sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrTVEc6byXAs",
        "outputId": "8a828d22-5d9e-42bd-8dc6-16598ac7c02d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-06 17:35:39--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: ‘cuda-ubuntu2004.pin’\n",
            "\n",
            "\rcuda-ubuntu2004.pin   0%[                    ]       0  --.-KB/s               \rcuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-08-06 17:35:39 (8.29 MB/s) - ‘cuda-ubuntu2004.pin’ saved [190/190]\n",
            "\n",
            "--2024-08-06 17:35:40--  https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2286573544 (2.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu200 100%[===================>]   2.13G  24.4MB/s    in 62s     \n",
            "\n",
            "2024-08-06 17:36:41 (35.4 MB/s) - ‘cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb’ saved [2286573544/2286573544]\n",
            "\n",
            "(Reading database ... 123598 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "Setting up cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "\n",
            "The public CUDA GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 file:/var/cuda-repo-ubuntu2004-11-3-local  Packages [30.3 kB]\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Ign:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-11.3-1' for regex 'cuda-11.3'\n",
            "Note, selecting 'cuda-11-3' for regex 'cuda-11.3'\n",
            "The following additional packages will be installed:\n",
            "  cpp-12 cuda-command-line-tools-11-3 cuda-compiler-11-3 cuda-cudart-11-3\n",
            "  cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3 cuda-cupti-dev-11-3\n",
            "  cuda-cuxxfilt-11-3 cuda-demo-suite-11-3 cuda-documentation-11-3\n",
            "  cuda-driver-dev-11-3 cuda-drivers cuda-drivers-560 cuda-gdb-11-3\n",
            "  cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-560 libnvidia-common-560\n",
            "  libnvidia-compute-560 libnvidia-decode-560 libnvidia-encode-560\n",
            "  libnvidia-extra-560 libnvidia-fbc1-560 libnvidia-gl-560 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libudev1 libxcvt0 libxfont2\n",
            "  libxkbfile1 libxtst6 libxxf86dga1 nsight-compute-2021.1.0\n",
            "  nsight-systems-2021.1.3 nvidia-compute-utils-560 nvidia-dkms-560\n",
            "  nvidia-driver-560 nvidia-firmware-560-560.28.03 nvidia-kernel-common-560\n",
            "  nvidia-kernel-source-560 nvidia-prime nvidia-settings nvidia-utils-560\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-560\n",
            "Suggested packages:\n",
            "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc gvfs\n",
            "  mesa-utils xfs | xserver xfonts-100dpi | xfonts-75dpi xfonts-scalable\n",
            "Recommended packages:\n",
            "  libnvidia-compute-560:i386 libnvidia-decode-560:i386\n",
            "  libnvidia-encode-560:i386 libnvidia-fbc1-560:i386 libnvidia-gl-560:i386\n",
            "The following NEW packages will be installed:\n",
            "  cpp-12 cuda-11-3 cuda-command-line-tools-11-3 cuda-compiler-11-3\n",
            "  cuda-cudart-11-3 cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3\n",
            "  cuda-cupti-dev-11-3 cuda-cuxxfilt-11-3 cuda-demo-suite-11-3\n",
            "  cuda-documentation-11-3 cuda-driver-dev-11-3 cuda-drivers cuda-drivers-560\n",
            "  cuda-gdb-11-3 cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-560 libnvidia-common-560\n",
            "  libnvidia-compute-560 libnvidia-decode-560 libnvidia-encode-560\n",
            "  libnvidia-extra-560 libnvidia-fbc1-560 libnvidia-gl-560 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libxcvt0 libxfont2 libxkbfile1\n",
            "  libxtst6 libxxf86dga1 nsight-compute-2021.1.0 nsight-systems-2021.1.3\n",
            "  nvidia-compute-utils-560 nvidia-dkms-560 nvidia-driver-560\n",
            "  nvidia-firmware-560-560.28.03 nvidia-kernel-common-560\n",
            "  nvidia-kernel-source-560 nvidia-prime nvidia-settings nvidia-utils-560\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-560\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 115 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 347 MB/2,384 MB of archives.\n",
            "After this operation, 5,452 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-11-3 11.3.58-1 [156 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-config-common 11.8.89-1 [16.4 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-560 560.28.03-0ubuntu1 [17.2 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-560 560.28.03-0ubuntu1 [48.4 MB]\n",
            "Get:5 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-11-3 11.3.58-1 [25.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-560 560.28.03-0ubuntu1 [140 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
            "Get:9 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-11-3 11.4.2.10064-1 [134 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-560 560.28.03-0ubuntu1 [50.8 MB]\n",
            "Get:12 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-11-3 10.4.2.58-1 [107 MB]\n",
            "Get:13 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-11-3 10.2.4.58-1 [40.0 MB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-560-560.28.03 560.28.03-0ubuntu1 [45.1 MB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-560 560.28.03-0ubuntu1 [109 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-560 560.28.03-0ubuntu1 [36.2 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-560 560.28.03-0ubuntu1 [73.0 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-560 560.28.03-0ubuntu1 [119 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-560 560.28.03-0ubuntu1 [1,911 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-560 560.28.03-0ubuntu1 [105 kB]\n",
            "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-560 560.28.03-0ubuntu1 [498 kB]\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-560 560.28.03-0ubuntu1 [146 kB]\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-560 560.28.03-0ubuntu1 [1,617 kB]\n",
            "Get:25 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-11-3 11.1.1.58-1 [75.7 MB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-560 560.28.03-0ubuntu1 [95.0 kB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-560 560.28.03-0ubuntu1 [496 kB]\n",
            "Get:28 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-560 560.28.03-1 [2,542 B]\n",
            "Get:29 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 560.28.03-1 [2,506 B]\n",
            "Get:30 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 560.28.03-0ubuntu1 [964 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
            "Get:34 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-11-3 11.5.0.58-1 [99.1 MB]\n",
            "Get:35 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-11-3 11.3.3.44-1 [72.3 MB]\n",
            "Get:36 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-11-3 11.4.1.58-1 [1,736 kB]\n",
            "Get:37 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-11-3 11.3.0-1 [2,502 B]\n",
            "Get:38 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-runtime-11-3 11.3.0-1 [2,420 B]\n",
            "Get:39 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuobjdump-11-3 11.3.58-1 [112 kB]\n",
            "Get:40 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuxxfilt-11-3 11.3.58-1 [44.1 kB]\n",
            "Get:41 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-thrust-11-3 11.3.58-1 [982 kB]\n",
            "Get:42 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-driver-dev-11-3 11.3.58-1 [26.2 kB]\n",
            "Get:43 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-dev-11-3 11.3.58-1 [737 kB]\n",
            "Get:44 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvcc-11-3 11.3.58-1 [45.7 MB]\n",
            "Get:45 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprune-11-3 11.3.58-1 [54.9 kB]\n",
            "Get:46 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-compiler-11-3 11.3.0-1 [2,428 B]\n",
            "Get:47 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-dev-11-3 11.3.58-1 [23.3 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.11 [1,477 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.24+8-1ubuntu3~22.04 [214 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:68 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-dev-11-3 11.4.2.10064-1 [141 MB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/main amd64 nvidia-prime all 0.8.17.1 [9,956 B]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:87 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-dev-11-3 10.4.2.58-1 [179 MB]\n",
            "Get:88 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-dev-11-3 10.2.4.58-1 [40.4 MB]\n",
            "Get:89 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-dev-11-3 11.1.1.58-1 [21.5 MB]\n",
            "Get:90 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-dev-11-3 11.5.0.58-1 [99.8 MB]\n",
            "Get:91 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-dev-11-3 11.3.3.44-1 [70.4 MB]\n",
            "Get:92 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-dev-11-3 11.4.1.58-1 [1,428 kB]\n",
            "Get:93 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-dev-11-3 11.3.0-1 [2,530 B]\n",
            "Get:94 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-11-3 11.3.58-1 [11.5 MB]\n",
            "Get:95 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-dev-11-3 11.3.58-1 [2,401 kB]\n",
            "Get:96 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvdisasm-11-3 11.3.58-1 [32.9 MB]\n",
            "Get:97 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-gdb-11-3 11.3.58-1 [3,622 kB]\n",
            "Get:98 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-memcheck-11-3 11.3.58-1 [145 kB]\n",
            "Get:99 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprof-11-3 11.3.58-1 [1,930 kB]\n",
            "Get:100 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvtx-11-3 11.3.58-1 [51.1 kB]\n",
            "Get:101 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-sanitizer-11-3 11.3.58-1 [7,534 kB]\n",
            "Get:102 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-command-line-tools-11-3 11.3.0-1 [2,466 B]\n",
            "Get:103 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-compute-2021.1.0 2021.1.0.18-1 [274 MB]\n",
            "Get:104 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-compute-11-3 11.3.0-1 [3,708 B]\n",
            "Get:105 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-systems-2021.1.3 2021.1.3.14-b695ea9 [248 MB]\n",
            "Get:106 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-systems-11-3 11.3.0-1 [3,302 B]\n",
            "Get:107 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-11-3 11.3.58-1 [119 MB]\n",
            "Get:108 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvml-dev-11-3 11.3.58-1 [73.3 kB]\n",
            "Get:109 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvvp-11-3 11.3.58-1 [115 MB]\n",
            "Get:110 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-visual-tools-11-3 11.3.0-1 [2,868 B]\n",
            "Get:111 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-tools-11-3 11.3.0-1 [2,380 B]\n",
            "Get:112 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-samples-11-3 11.3.58-1 [59.2 MB]\n",
            "Get:113 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-documentation-11-3 11.3.58-1 [48.2 kB]\n",
            "Get:114 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-toolkit-11-3 11.3.0-1 [3,274 B]\n",
            "Get:115 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-demo-suite-11-3 11.3.58-1 [3,978 kB]\n",
            "Get:116 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-11-3 11.3.0-1 [2,450 B]\n",
            "Fetched 347 MB in 16s (21.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 116.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package liblocale-gettext-perl.\n",
            "(Reading database ... 123710 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
            "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
            "Selecting previously unselected package keyboard-configuration.\n",
            "Preparing to unpack .../1-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
            "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
            "Selecting previously unselected package cpp-12.\n",
            "Preparing to unpack .../2-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan8:amd64.\n",
            "Preparing to unpack .../3-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libtsan2:amd64.\n",
            "Preparing to unpack .../4-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-12-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-12.\n",
            "Preparing to unpack .../6-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package dctrl-tools.\n",
            "Preparing to unpack .../7-dctrl-tools_2.24-3build2_amd64.deb ...\n",
            "Unpacking dctrl-tools (2.24-3build2) ...\n",
            "Selecting previously unselected package dkms.\n",
            "Preparing to unpack .../8-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
            "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
            "Preparing to unpack .../9-libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 124070 files and directories currently installed.)\n",
            "Preparing to unpack .../000-udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "Preparing to unpack .../001-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-config-common.\n",
            "Preparing to unpack .../002-cuda-toolkit-11-config-common_11.8.89-1_all.deb ...\n",
            "Unpacking cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-11-3.\n",
            "Preparing to unpack .../003-cuda-cudart-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-3.\n",
            "Preparing to unpack .../004-cuda-nvrtc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-11-3.\n",
            "Preparing to unpack .../005-libcublas-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-11-3.\n",
            "Preparing to unpack .../006-libcufft-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-11-3.\n",
            "Preparing to unpack .../007-libcurand-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-11-3.\n",
            "Preparing to unpack .../008-libcusolver-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-11-3.\n",
            "Preparing to unpack .../009-libcusparse-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-11-3.\n",
            "Preparing to unpack .../010-libnpp-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-3.\n",
            "Preparing to unpack .../011-libnvjpeg-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-3.\n",
            "Preparing to unpack .../012-cuda-libraries-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libnvidia-common-560.\n",
            "Preparing to unpack .../013-libnvidia-common-560_560.28.03-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-560:amd64.\n",
            "Preparing to unpack .../014-libnvidia-compute-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-gl-560:amd64.\n",
            "Preparing to unpack .../015-libnvidia-gl-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-535\n",
            "Unpacking libnvidia-gl-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-source-560.\n",
            "Preparing to unpack .../016-nvidia-kernel-source-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-firmware-560-560.28.03.\n",
            "Preparing to unpack .../017-nvidia-firmware-560-560.28.03_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-firmware-560-560.28.03 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-common-560.\n",
            "Preparing to unpack .../018-nvidia-kernel-common-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-dkms-560.\n",
            "Preparing to unpack .../019-nvidia-dkms-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-dkms-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-extra-560:amd64.\n",
            "Preparing to unpack .../020-libnvidia-extra-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-extra-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-560.\n",
            "Preparing to unpack .../021-nvidia-compute-utils-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-decode-560:amd64.\n",
            "Preparing to unpack .../022-libnvidia-decode-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-560:amd64.\n",
            "Preparing to unpack .../023-libnvidia-encode-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-560.\n",
            "Preparing to unpack .../024-nvidia-utils-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-utils-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-cfg1-560:amd64.\n",
            "Preparing to unpack .../025-libnvidia-cfg1-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../026-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../027-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../028-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package libxcvt0:amd64.\n",
            "Preparing to unpack .../029-libxcvt0_0.1.1-3_amd64.deb ...\n",
            "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../030-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../031-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package xserver-xorg-core.\n",
            "Preparing to unpack .../032-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xserver-xorg-video-nvidia-560.\n",
            "Preparing to unpack .../033-xserver-xorg-video-nvidia-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-fbc1-560:amd64.\n",
            "Preparing to unpack .../034-libnvidia-fbc1-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-560.\n",
            "Preparing to unpack .../035-nvidia-driver-560_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-560 (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package cuda-drivers-560.\n",
            "Preparing to unpack .../036-cuda-drivers-560_560.28.03-1_amd64.deb ...\n",
            "Unpacking cuda-drivers-560 (560.28.03-1) ...\n",
            "Selecting previously unselected package cuda-drivers.\n",
            "Preparing to unpack .../037-cuda-drivers_560.28.03-1_amd64.deb ...\n",
            "Unpacking cuda-drivers (560.28.03-1) ...\n",
            "Selecting previously unselected package cuda-runtime-11-3.\n",
            "Preparing to unpack .../038-cuda-runtime-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-3.\n",
            "Preparing to unpack .../039-cuda-cuobjdump-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-11-3.\n",
            "Preparing to unpack .../040-cuda-cuxxfilt-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-thrust-11-3.\n",
            "Preparing to unpack .../041-cuda-thrust-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-3.\n",
            "Preparing to unpack .../042-cuda-driver-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-3.\n",
            "Preparing to unpack .../043-cuda-cudart-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-3.\n",
            "Preparing to unpack .../044-cuda-nvcc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-3.\n",
            "Preparing to unpack .../045-cuda-nvprune-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-3.\n",
            "Preparing to unpack .../046-cuda-compiler-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-3.\n",
            "Preparing to unpack .../047-cuda-nvrtc-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-3.\n",
            "Preparing to unpack .../048-libcublas-dev-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-3.\n",
            "Preparing to unpack .../049-libcufft-dev-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-3.\n",
            "Preparing to unpack .../050-libcurand-dev-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-3.\n",
            "Preparing to unpack .../051-libcusolver-dev-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-3.\n",
            "Preparing to unpack .../052-libcusparse-dev-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-3.\n",
            "Preparing to unpack .../053-libnpp-dev-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-3.\n",
            "Preparing to unpack .../054-libnvjpeg-dev-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-3.\n",
            "Preparing to unpack .../055-cuda-libraries-dev-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-3.\n",
            "Preparing to unpack .../056-cuda-cupti-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-3.\n",
            "Preparing to unpack .../057-cuda-cupti-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-3.\n",
            "Preparing to unpack .../058-cuda-nvdisasm-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-3.\n",
            "Preparing to unpack .../059-cuda-gdb-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-3.\n",
            "Preparing to unpack .../060-cuda-memcheck-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-3.\n",
            "Preparing to unpack .../061-cuda-nvprof-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-3.\n",
            "Preparing to unpack .../062-cuda-nvtx-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-3.\n",
            "Preparing to unpack .../063-cuda-sanitizer-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-3.\n",
            "Preparing to unpack .../064-cuda-command-line-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-compute-2021.1.0.\n",
            "Preparing to unpack .../065-nsight-compute-2021.1.0_2021.1.0.18-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-3.\n",
            "Preparing to unpack .../066-cuda-nsight-compute-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-systems-2021.1.3.\n",
            "Preparing to unpack .../067-nsight-systems-2021.1.3_2021.1.3.14-1_amd64.deb ...\n",
            "Unpacking nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-3.\n",
            "Preparing to unpack .../068-cuda-nsight-systems-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../069-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../070-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../071-openjdk-11-jre_11.0.24+8-1ubuntu3~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../072-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-11-3.\n",
            "Preparing to unpack .../073-cuda-nsight-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-3.\n",
            "Preparing to unpack .../074-cuda-nvml-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-3.\n",
            "Preparing to unpack .../075-cuda-nvvp-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-3.\n",
            "Preparing to unpack .../076-cuda-visual-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-3.\n",
            "Preparing to unpack .../077-cuda-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-samples-11-3.\n",
            "Preparing to unpack .../078-cuda-samples-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-samples-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-3.\n",
            "Preparing to unpack .../079-cuda-documentation-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-3.\n",
            "Preparing to unpack .../080-cuda-toolkit-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-11-3.\n",
            "Preparing to unpack .../081-cuda-demo-suite-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-11-3.\n",
            "Preparing to unpack .../082-cuda-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libfakeroot:amd64.\n",
            "Preparing to unpack .../083-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fakeroot.\n",
            "Preparing to unpack .../084-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../085-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../086-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../087-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../088-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../089-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../090-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../091-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../092-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../093-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../094-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../095-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../096-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package nvidia-prime.\n",
            "Preparing to unpack .../097-nvidia-prime_0.8.17.1_all.deb ...\n",
            "Unpacking nvidia-prime (0.8.17.1) ...\n",
            "Selecting previously unselected package python3-xkit.\n",
            "Preparing to unpack .../098-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
            "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
            "Selecting previously unselected package screen-resolution-extra.\n",
            "Preparing to unpack .../099-screen-resolution-extra_0.18.2_all.deb ...\n",
            "Unpacking screen-resolution-extra (0.18.2) ...\n",
            "Selecting previously unselected package nvidia-settings.\n",
            "Preparing to unpack .../100-nvidia-settings_560.28.03-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-settings (560.28.03-0ubuntu1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../101-systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Selecting previously unselected package xcvt.\n",
            "Preparing to unpack .../102-xcvt_0.1.1-3_amd64.deb ...\n",
            "Unpacking xcvt (0.1.1-3) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../103-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../104-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../105-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Setting up libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Setting up libnvidia-fbc1-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Setting up cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Setting up cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Setting up cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Setting up nvidia-prime (0.8.17.1) ...\n",
            "Setting up cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libnvidia-compute-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Setting up libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Setting up libnvidia-common-560 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Setting up nvidia-kernel-source-560 (560.28.03-0ubuntu1) ...\n",
            "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Setting up fakeroot (1.28-1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
            "Setting up cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-cfg1-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Setting up libnvidia-extra-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up nvidia-firmware-560-560.28.03 (560.28.03-0ubuntu1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libnpp-11-3 (11.3.3.44-1) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Setting up libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Setting up nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: error: alternative path /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsight-sys doesn't exist\n",
            "update-alternatives: error: no alternatives for nsight-sys\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
            "Setting up libnvidia-gl-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libcurand-11-3 (10.2.4.58-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Setting up libcufft-11-3 (10.4.2.58-1) ...\n",
            "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
            "Setting up libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Setting up cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
            "Setting up dctrl-tools (2.24-3build2) ...\n",
            "Setting up libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Setting up nvidia-utils-560 (560.28.03-0ubuntu1) ...\n",
            "Setting up libnvidia-decode-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up nvidia-compute-utils-560 (560.28.03-0ubuntu1) ...\n",
            "Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory\n",
            "Adding system user `nvidia-persistenced' (UID 104) ...\n",
            "Adding new group `nvidia-persistenced' (GID 111) ...\n",
            "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
            "Not creating home directory `/nonexistent'.\n",
            "Setting up cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Setting up cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Setting up cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Setting up libnvidia-encode-560:amd64 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Setting up nvidia-kernel-common-560 (560.28.03-0ubuntu1) ...\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-hibernate.service → /lib/systemd/system/nvidia-hibernate.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-resume.service → /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-resume.service → /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-suspend.service → /lib/systemd/system/nvidia-suspend.service.\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Setting up xcvt (0.1.1-3) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Setting up screen-resolution-extra (0.18.2) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up nvidia-settings (560.28.03-0ubuntu1) ...\n",
            "Setting up libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Configuring keyboard-configuration\n",
            "----------------------------------\n",
            "\n",
            "The layout of keyboards varies per country, with some countries having multiple\n",
            "common layouts. Please select the country of origin for the keyboard of this\n",
            "computer.\n",
            "\n",
            "  1. Afghani\n",
            "  2. Albanian\n",
            "  3. Amharic\n",
            "  4. Arabic\n",
            "  5. Arabic (Morocco)\n",
            "  6. Arabic (Syria)\n",
            "  7. Armenian\n",
            "  8. A user-defined custom Layout\n",
            "  9. Azerbaijani\n",
            "  10. Bambara\n",
            "  11. Bangla\n",
            "  12. Belarusian\n",
            "  13. Belgian\n",
            "  14. Berber (Algeria, Latin)\n",
            "  15. Bosnian\n",
            "  16. Braille\n",
            "  17. Bulgarian\n",
            "  18. Burmese\n",
            "  19. Chinese\n",
            "  20. Croatian\n",
            "  21. Czech\n",
            "  22. Danish\n",
            "  23. Dhivehi\n",
            "  24. Dutch\n",
            "  25. Dzongkha\n",
            "  26. English (Australian)\n",
            "  27. English (Cameroon)\n",
            "  28. English (Ghana)\n",
            "  29. English (Nigeria)\n",
            "  30. English (South Africa)\n",
            "  31. English (UK)\n",
            "  32. English (US)\n",
            "  33. Esperanto\n",
            "  34. Estonian\n",
            "  35. Faroese\n",
            "  36. Filipino\n",
            "  37. Finnish\n",
            "  38. French\n",
            "  39. French (Canada)\n",
            "  40. French (Democratic Republic of the Congo)\n",
            "  41. French (Togo)\n",
            "  42. Georgian\n",
            "  43. German\n",
            "  44. German (Austria)\n",
            "  45. Greek\n",
            "  46. Hebrew\n",
            "  47. Hungarian\n",
            "  48. Icelandic\n",
            "  49. Indian\n",
            "  50. Indonesian (Javanese)\n",
            "  51. Indonesian (Latin)\n",
            "  52. Iraqi\n",
            "  53. Irish\n",
            "  54. Italian\n",
            "  55. Japanese\n",
            "  56. Japanese (PC-98)\n",
            "  57. Kazakh\n",
            "  58. Khmer (Cambodia)\n",
            "  59. Korean\n",
            "  60. Kyrgyz\n",
            "  61. Lao\n",
            "  62. Latvian\n",
            "  63. Lithuanian\n",
            "  64. Macedonian\n",
            "  65. Malay (Jawi, Arabic Keyboard)\n",
            "  66. Maltese\n",
            "  67. Maori\n",
            "  68. Moldavian\n",
            "  69. Mongolian\n",
            "  70. Montenegrin\n",
            "  71. Nepali\n",
            "  72. NKo (AZERTY)\n",
            "  73. Norwegian\n",
            "  74. Persian\n",
            "  75. Polish\n",
            "  76. Portuguese\n",
            "  77. Portuguese (Brazil)\n",
            "  78. Romanian\n",
            "  79. Russian\n",
            "  80. Serbian\n",
            "  81. Sinhala (phonetic)\n",
            "  82. Slovak\n",
            "  83. Slovenian\n",
            "  84. Spanish\n",
            "  85. Spanish (Latin American)\n",
            "  86. Swahili (Kenya)\n",
            "  87. Swahili (Tanzania)\n",
            "  88. Swedish\n",
            "  89. Switzerland\n",
            "  90. Taiwanese\n",
            "  91. Tajik\n",
            "  92. Thai\n",
            "  93. Tswana\n",
            "  94. Turkish\n",
            "  95. Turkmen\n",
            "  96. Ukrainian\n",
            "  97. Urdu (Pakistan)\n",
            "  98. Uzbek\n",
            "  99. Vietnamese\n",
            "  100. Wolof\n",
            "\u001b[4mCountry of origin for the keyboard: \u001b[m\u001b[1m32\n",
            "\u001b[m\u001b[m\n",
            "Please select the layout matching the keyboard for this machine.\n",
            "\n",
            "  1. English (US)\n",
            "  2. English (US) - Cherokee\n",
            "  3. English (US) - English (classic Dvorak)\n",
            "  4. English (US) - English (Colemak)\n",
            "  5. English (US) - English (Colemak-DH)\n",
            "  6. English (US) - English (Colemak-DH ISO)\n",
            "  7. English (US) - English (Dvorak)\n",
            "  8. English (US) - English (Dvorak, alt. intl.)\n",
            "  9. English (US) - English (Dvorak, intl., with dead keys)\n",
            "  10. English (US) - English (Dvorak, left-handed)\n",
            "  11. English (US) - English (Dvorak, right-handed)\n",
            "  12. English (US) - English (intl., with AltGr dead keys)\n",
            "  13. English (US) - English (Macintosh)\n",
            "  14. English (US) - English (Norman)\n",
            "  15. English (US) - English (programmer Dvorak)\n",
            "  16. English (US) - English (the divide/multiply toggle the layout)\n",
            "  17. English (US) - English (US, alt. intl.)\n",
            "  18. English (US) - English (US, euro on 5)\n",
            "  19. English (US) - English (US, intl., with dead keys)\n",
            "  20. English (US) - English (US, Symbolic)\n",
            "  21. English (US) - English (Workman)\n",
            "  22. English (US) - English (Workman, intl., with dead keys)\n",
            "  23. English (US) - Hawaiian\n",
            "  24. English (US) - Russian (US, phonetic)\n",
            "  25. English (US) - Serbo-Croatian (US)\n",
            "\u001b[4mKeyboard layout: \u001b[m\u001b[1m1\n",
            "\u001b[m\u001b[m\n",
            "Your console font configuration will be updated the next time your system\n",
            "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
            "Setting up cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Setting up xserver-xorg-video-nvidia-560 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-samples-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Setting up dkms (2.8.7-2ubuntu2.2) ...\n",
            "Setting up cuda-tools-11-3 (11.3.0-1) ...\n",
            "Setting up nvidia-dkms-560 (560.28.03-0ubuntu1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Loading new nvidia-560.28.03 DKMS files...\n",
            "It is likely that 6.1.85+ belongs to a chroot's host\n",
            "Building for 5.15.0-117-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 5.15.0-117-generic\n",
            "Done.\n",
            "\n",
            "nvidia.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-peermem.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "Setting up cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Setting alternatives\n",
            "update-alternatives: using /usr/local/cuda-11.3 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
            "Setting up nvidia-driver-560 (560.28.03-0ubuntu1) ...\n",
            "Setting up cuda-drivers-560 (560.28.03-1) ...\n",
            "Setting up cuda-drivers (560.28.03-1) ...\n",
            "Setting up cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-11-3 (11.3.0-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUDA_PATH=/usr/local/cuda-11.3/\n",
        "#!export PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/"
      ],
      "metadata": {
        "id": "PuthK8ce16jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claude says that the !export doesn't persist outside of the cell. Whoops."
      ],
      "metadata": {
        "id": "qzBJtnmg9CcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.3'\n",
        "os.environ['PATH'] = '/usr/local/cuda-11.3/bin:' + os.environ['PATH']\n",
        "\n",
        "# THIS WORKED! YAY!!\n",
        "\n",
        "! nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBSdi8Nt9FoS",
        "outputId": "d5c35a07-0fee-4ca4-f505-fd0eae5a5486"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek_OMQGz8aeR",
        "outputId": "ea7be821-c036-49c2-c2eb-3830453813e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display cuda\n",
        "\n",
        "!sudo update-alternatives --config cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2k5e63-Uwg",
        "outputId": "3457f8fd-de3b-48b6-b4fa-fb8a0ee72bea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda - manual mode\n",
            "  link best version is /usr/local/cuda-12.2\n",
            "  link currently points to /usr/local/cuda-11.3\n",
            "  link cuda is /usr/local/cuda\n",
            "/usr/local/cuda-11.3 - priority 113\n",
            "/usr/local/cuda-12.2 - priority 122\n",
            "There are 2 choices for the alternative cuda (providing /usr/local/cuda).\n",
            "\n",
            "  Selection    Path                  Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/local/cuda-12.2   122       auto mode\n",
            "* 1            /usr/local/cuda-11.3   113       manual mode\n",
            "  2            /usr/local/cuda-12.2   122       manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we install a GCC version compatible:"
      ],
      "metadata": {
        "id": "PorxhJIg-1q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check(?)\n",
        "\n",
        "!sudo update-alternatives --remove-all gcc\n",
        "!sudo update-alternatives --remove-all g++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v3F4i02JK1g",
        "outputId": "5db1abaa-4746-4bfe-c08d-09f0f330278d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: error: no alternatives for gcc\n",
            "update-alternatives: error: no alternatives for g++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential software-properties-common -y\n",
        "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc-9 g++-9 -y\n",
        "!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9\n",
        "!gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GaBumXpw-1TR",
        "outputId": "2fe3bab4-0dfc-4df3-d0f1-b1f95dc1a794"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r0% [1 InRelease 0 B]\r                    \rIgn:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "PPA publishes dbgsym, you may need to include 'main/debug' component\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu/ jammy main'\n",
            "Description:\n",
            "Toolchain test builds; see https://wiki.ubuntu.com/ToolChain\n",
            "\n",
            "More info: https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/ubuntu-toolchain-r-ubuntu-test.gpg with fingerprint C8EC952E2A0E1FBDC5090F6A2C277A0A352154E5\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:13 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease [24.6 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy/main amd64 Packages [17.4 kB]\n",
            "Fetched 42.0 kB in 2s (25.8 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:14 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cpp-9 gcc-9-base libasan5 libgcc-9-dev libstdc++-9-dev\n",
            "Suggested packages:\n",
            "  gcc-9-locales g++-9-multilib gcc-9-doc gcc-9-multilib libstdc++-9-doc\n",
            "The following NEW packages will be installed:\n",
            "  cpp-9 g++-9 gcc-9 gcc-9-base libasan5 libgcc-9-dev libstdc++-9-dev\n",
            "0 upgraded, 7 newly installed, 0 to remove and 63 not upgraded.\n",
            "Need to get 41.2 MB of archives.\n",
            "After this operation, 138 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gcc-9-base amd64 9.5.0-1ubuntu1~22.04 [19.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 cpp-9 amd64 9.5.0-1ubuntu1~22.04 [10.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libasan5 amd64 9.5.0-1ubuntu1~22.04 [3,140 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgcc-9-dev amd64 9.5.0-1ubuntu1~22.04 [2,520 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gcc-9 amd64 9.5.0-1ubuntu1~22.04 [11.3 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libstdc++-9-dev amd64 9.5.0-1ubuntu1~22.04 [1,824 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 g++-9 amd64 9.5.0-1ubuntu1~22.04 [11.9 MB]\n",
            "Fetched 41.2 MB in 5s (7,679 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-9-base:amd64.\n",
            "(Reading database ... 132537 files and directories currently installed.)\n",
            "Preparing to unpack .../0-gcc-9-base_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-9-base:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package cpp-9.\n",
            "Preparing to unpack .../1-cpp-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan5:amd64.\n",
            "Preparing to unpack .../2-libasan5_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan5:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-9-dev:amd64.\n",
            "Preparing to unpack .../3-libgcc-9-dev_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-9.\n",
            "Preparing to unpack .../4-gcc-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libstdc++-9-dev:amd64.\n",
            "Preparing to unpack .../5-libstdc++-9-dev_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libstdc++-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package g++-9.\n",
            "Preparing to unpack .../6-g++-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking g++-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up gcc-9-base:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libasan5:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up cpp-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libgcc-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up gcc-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libstdc++-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up g++-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "update-alternatives: using /usr/bin/gcc-9 to provide /usr/bin/gcc (gcc) in auto mode\n",
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fixed it again! yay!\n",
        "import os\n",
        "os.environ['CC'] = '/usr/bin/gcc-9'\n",
        "os.environ['CXX'] = '/usr/bin/g++-9'\n",
        "os.environ['CUDAHOSTCXX'] = '/usr/bin/g++-9'"
      ],
      "metadata": {
        "id": "yYnMcbidHsgR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works, but you will have to deal with this girthy installation every time you want to use this notebook."
      ],
      "metadata": {
        "id": "VQUnsKUL9SMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning and installing torch-int repo"
      ],
      "metadata": {
        "id": "HtSDBgz71IKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1188LC3LtZYt",
        "outputId": "621aa125-de0c-4604-dae5-efab7438fcba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Guangxuan-Xiao/torch-int.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4AytQdgi_gj",
        "outputId": "f4b53035-0c83-4f3e-d319-af55895955a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-int'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1102 (delta 139), reused 118 (delta 118), pack-reused 929\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 960.91 KiB | 31.00 MiB/s, done.\n",
            "Resolving deltas: 100% (643/643), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torch-int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjJqyK7jwSP",
        "outputId": "e793169c-15fb-44a3-c68c-31976e9b75a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also had to jump through some hoops trying to figure out this submodule stuff—it is not exactly what the repo says to do"
      ],
      "metadata": {
        "id": "FbQYofhC94cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is probably where the fix is going to be\n",
        "\n",
        "! git config submodule.submodules/cutlass.url https://github.com/NVIDIA/cutlass.git\n",
        "! git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI103aFzp_RG",
        "outputId": "a937c6fa-3a96-4d11-c6d1-cb6d63ea9419"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/torch-int/submodules/cutlass'...\n",
            "Submodule path 'submodules/cutlass': checked out 'c975e2ccbb2dbf13024568b37ffa3498ed0b3aed'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2R7WxtNCBYZ",
        "outputId": "28d2a29a-7cba-4cbf-a794-78f279f28a52"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yet, when I try to build_cutlass.sh, the CUDA compiler is still identified as 12.2.140. The way to fix this is the cell above, changing the alternatives for CUDA usage"
      ],
      "metadata": {
        "id": "oREC2Ch-9xRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display gcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1zmiVBCElD",
        "outputId": "2192155e-1bac-42ee-99a9-266eca95fdf7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc - auto mode\n",
            "  link best version is /usr/bin/gcc-9\n",
            "  link currently points to /usr/bin/gcc-9\n",
            "  link gcc is /usr/bin/gcc\n",
            "  slave g++ is /usr/bin/g++\n",
            "/usr/bin/gcc-9 - priority 90\n",
            "  slave g++: /usr/bin/g++-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This still gave me errors for a bit, because you need to use a GCC compatible with the different CUDA version. But even when I installed a new GCC version, it seemed to use it but didn't? Weird."
      ],
      "metadata": {
        "id": "LM_FmyANCmtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! source environment.sh\n",
        "! CC=/usr/bin/gcc-9 CXX=/usr/bin/g++-9 bash build_cutlass.sh   # this one needs GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DSE8hTjCIU",
        "outputId": "99129f83-6a55-4f4f-afba-357a43cac5e1",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- CMake Version: 3.30.1\n",
            "-- The CXX compiler identification is GNU 9.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 11.3.58\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- CUDART: /usr/local/cuda/lib64/libcudart.so\n",
            "-- CUDA Driver: /usr/local/cuda/lib64/stubs/libcuda.so\n",
            "-- NVRTC: /usr/local/cuda/lib64/libnvrtc.so\n",
            "-- Default Install Location: install\n",
            "-- CUDA Compilation Architectures: 80\n",
            "-- Enable caching of reference results in conv unit tests\n",
            "-- Enable rigorous conv problem sizes in conv unit tests\n",
            "-- Using NVCC flags: -DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;$<$<BOOL:1>:-Xcompiler=-Wconversion>;$<$<BOOL:1>:-Xcompiler=-fno-strict-aliasing>\n",
            "-- CUTLASS Revision: c975e2cc\n",
            "-- Configuring cublas ...\n",
            "-- cuBLAS Disabled.\n",
            "-- Configuring cuBLAS ... done.\n",
            "-- Found Python3: /usr/local/bin/python (found suitable version \"3.10.12\", minimum required is \"3.5\") found components: Interpreter\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c09b164b76dc.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.1134c512275c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.70bca42e032c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7412477fb00b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7cb784ae67c3.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.452e17cedfde.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.21cd46163fea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.f2656f9851f5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.da510d6f1a23.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed2bb5981d55.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.22f9e56a890e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.17417f539ca2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.d1a5010bad63.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5a217a522f90.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e764a5c32d2e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.00a490a48b54.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.67748a1f69fd.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.aaae24350764.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a7556fb0c363.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.cdd7679105ea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5b5abd9da9bb.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0188c1e5012f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c1136defa56d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.4efc263febd5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b1c066c89ad1.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.059188bf387f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dc6fdf4e3631.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed319cdaa486.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7f69d119038b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ab4c8725d89a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.db0920d13551.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.53863dce2b34.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5206989612aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a40fdb431023.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.999493302618.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.bbd766ae1258.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.3aca44f87dc7.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.fe5af8d45b59.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2d9ae5d87c7a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8112247fe822.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.447ac2531506.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b7cdd3426b6c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b83304556123.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c2bae868a773.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.51d0f833ea6d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dd6b23b6f933.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.eac06d5a86ae.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ec9d28d7c560.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.25dee0d0208a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.41f602329ae6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.9763c0db7135.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2fdf36a298c6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0a8164c28c5f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a4cfc29c733f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8a2ca8aaf7aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.977a56f2bc38.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e837fe821ee2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.6f06994db371.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ae96169b4e45.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a6a0ceb84086.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.19976851ec6d.cu\n",
            "-- Configuring done (3.5s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/torch-int/submodules/cutlass/build\n",
            "[  0%] \u001b[32mBuilding CUDA object examples/00_basic_gemm/CMakeFiles/00_basic_gemm.dir/basic_gemm.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/01_cutlass_utilities/CMakeFiles/01_cutlass_utilities.dir/cutlass_utilities.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/02_dump_reg_shmem/CMakeFiles/02_dump_reg_shmem.dir/dump_reg_shmem.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/visualize_layout.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/04_tile_iterator/CMakeFiles/04_tile_iterator.dir/tile_iterator.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/register_layout.cu.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CUDA object examples/05_batched_gemm/CMakeFiles/05_batched_gemm.dir/batched_gemm.cu.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object examples/06_splitK_gemm/CMakeFiles/06_splitK_gemm.dir/splitk_gemm.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object examples/07_volta_tensorop_gemm/CMakeFiles/07_volta_tensorop_gemm.dir/volta_tensorop_gemm.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object examples/08_turing_tensorop_gemm/CMakeFiles/08_turing_tensorop_gemm.dir/turing_tensorop_gemm.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/09_turing_tensorop_conv2dfprop/CMakeFiles/09_turing_tensorop_conv2dfprop.dir/turing_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/handle.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/12_gemm_bias_relu/CMakeFiles/12_gemm_bias_relu.dir/gemm_bias_relu.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_shmem.dir/fused_two_convs_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_rf.dir/fused_two_convs_f16_sm75_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_rf.dir/fused_two_convs_f16_sm80_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/src/manifest.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/operation_table.cu.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CUDA executable 04_tile_iterator\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/singleton.cu.o\u001b[0m\n",
            "[  9%] Built target 04_tile_iterator\n",
            "[ 10%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_shmem.dir/fused_two_convs_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CUDA executable 02_dump_reg_shmem\u001b[0m\n",
            "[ 10%] Built target 02_dump_reg_shmem\n",
            "[ 11%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_rf.dir/fused_two_convs_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/util.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CXX executable 03_visualize_layout\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_shmem.dir/fused_two_convs_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 14%] Built target 03_visualize_layout\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CUDA executable 08_turing_tensorop_gemm\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_rf.dir/fused_two_convs_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CUDA executable 00_basic_gemm\u001b[0m\n",
            "[ 15%] Built target 00_basic_gemm\n",
            "[ 15%] Built target 08_turing_tensorop_gemm\n",
            "[ 15%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/gemm.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_shmem.dir/fused_two_convs_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 12_gemm_bias_relu\u001b[0m\n",
            "[ 17%] Built target 12_gemm_bias_relu\n",
            "[ 18%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/initialize_reference_operations.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/reduction_device.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CUDA executable 01_cutlass_utilities\u001b[0m\n",
            "[ 18%] Built target 01_cutlass_utilities\n",
            "[ 18%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_rf.dir/fused_two_gemms_f16_sm75_rf.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CUDA executable 09_turing_tensorop_conv2dfprop\u001b[0m\n",
            "[ 18%] Built target 09_turing_tensorop_conv2dfprop\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/init_reduction_operations.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 05_batched_gemm\u001b[0m\n",
            "[ 19%] Built target 05_batched_gemm\n",
            "[ 19%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_shmem.dir/fused_two_gemms_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv2d.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv3d.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/generated/initialize_all.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c09b164b76dc.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_rf\u001b[0m\n",
            "[ 21%] Built target 13_fused_two_convs_f16_sm75_rf\n",
            "[ 21%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_rf.dir/fused_two_gemms_f16_sm80_rf.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_rf\u001b[0m\n",
            "[ 21%] Built target 13_fused_two_convs_f16_sm80_rf\n",
            "[ 22%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.1134c512275c.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_shmem\u001b[0m\n",
            "[ 23%] Built target 13_fused_two_convs_f16_sm75_shmem\n",
            "[ 23%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_shmem.dir/fused_two_gemms_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_shmem\u001b[0m\n",
            "[ 23%] Built target 13_fused_two_convs_f16_sm80_shmem\n",
            "[ 23%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_rf.dir/fused_two_gemms_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_rf\u001b[0m\n",
            "[ 23%] Built target 13_fused_two_convs_s8_sm75_rf\n",
            "[ 23%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.70bca42e032c.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_rf\u001b[0m\n",
            "[ 24%] Built target 13_fused_two_gemms_f16_sm75_rf\n",
            "[ 25%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7412477fb00b.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_shmem\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_rf\u001b[0m\n",
            "[ 26%] Built target 13_fused_two_convs_s8_sm80_rf\n",
            "[ 27%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_shmem.dir/fused_two_gemms_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 27%] Built target 13_fused_two_gemms_f16_sm75_shmem\n",
            "[ 28%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_rf.dir/fused_two_gemms_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_shmem\u001b[0m\n",
            "[ 28%] Built target 13_fused_two_convs_s8_sm75_shmem\n",
            "[ 29%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_shmem.dir/fused_two_gemms_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_shmem\u001b[0m\n",
            "[ 29%] Built target 13_fused_two_convs_s8_sm80_shmem\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/14_ampere_tf32_tensorop_gemm/CMakeFiles/14_ampere_tf32_tensorop_gemm.dir/ampere_tf32_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_rf\u001b[0m\n",
            "[ 31%] Built target 13_fused_two_gemms_f16_sm80_rf\n",
            "[ 32%] \u001b[32mBuilding CUDA object examples/15_ampere_sparse_tensorop_gemm/CMakeFiles/15_ampere_sparse_tensorop_gemm.dir/ampere_sparse_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_shmem\u001b[0m\n",
            "[ 33%] Built target 13_fused_two_gemms_f16_sm80_shmem\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/16_ampere_tensorop_conv2dfprop/CMakeFiles/16_ampere_tensorop_conv2dfprop.dir/ampere_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_rf\u001b[0m\n",
            "[ 33%] Built target 13_fused_two_gemms_s8_sm75_rf\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/17_fprop_per_channel_bias/CMakeFiles/17_fprop_per_channel_bias.dir/fprop_per_channel_bias.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 14_ampere_tf32_tensorop_gemm\u001b[0m\n",
            "[ 33%] Built target 14_ampere_tf32_tensorop_gemm\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeFiles/18_ampere_fp64_tensorop_affine2_gemm.dir/ampere_fp64_tensorop_affine2_gemm.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 15_ampere_sparse_tensorop_gemm\u001b[0m\n",
            "[ 33%] Built target 15_ampere_sparse_tensorop_gemm\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/19_tensorop_canonical/CMakeFiles/19_tensorop_canonical.dir/tensorop_canonical.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/20_simt_canonical/CMakeFiles/20_simt_canonical.dir/simt_canonical.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_shmem\u001b[0m\n",
            "[ 33%] Built target 13_fused_two_gemms_s8_sm75_shmem\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/21_quaternion_gemm/CMakeFiles/21_quaternion_gemm.dir/quaternion_gemm.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_rf\u001b[0m\n",
            "[ 33%] Built target 13_fused_two_gemms_s8_sm80_rf\n",
            "[ 34%] \u001b[32mBuilding CUDA object examples/22_quaternion_conv/CMakeFiles/22_quaternion_conv.dir/quaternion_conv.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_shmem\u001b[0m\n",
            "[ 34%] Built target 13_fused_two_gemms_s8_sm80_shmem\n",
            "[ 35%] \u001b[32mBuilding CUDA object examples/23_ampere_gemm_operand_reduction_fusion/CMakeFiles/23_ampere_gemm_operand_reduction_fusion.dir/ampere_gemm_operand_reduction_fusion.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA executable 16_ampere_tensorop_conv2dfprop\u001b[0m\n",
            "[ 36%] Built target 16_ampere_tensorop_conv2dfprop\n",
            "[ 37%] \u001b[32mBuilding CUDA object examples/24_gemm_grouped/CMakeFiles/24_gemm_grouped.dir/gemm_grouped.cu.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CUDA executable 17_fprop_per_channel_bias\u001b[0m\n",
            "[ 38%] Built target 17_fprop_per_channel_bias\n",
            "[ 38%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7cb784ae67c3.cu.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CUDA executable 19_tensorop_canonical\u001b[0m\n",
            "[ 39%] Built target 19_tensorop_canonical\n",
            "[ 40%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_fprop_mainloop_fusion.dir/ampere_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.452e17cedfde.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 20_simt_canonical\u001b[0m\n",
            "[ 42%] Built target 20_simt_canonical\n",
            "[ 43%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_3d_fprop_mainloop_fusion.dir/ampere_3d_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 18_ampere_fp64_tensorop_affine2_gemm\u001b[0m\n",
            "[ 44%] Built target 18_ampere_fp64_tensorop_affine2_gemm\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/26_ampere_wgrad_mainloop_fusion/CMakeFiles/26_ampere_wgrad_mainloop_fusion.dir/ampere_wgrad_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 21_quaternion_gemm\u001b[0m\n",
            "[ 44%] Built target 21_quaternion_gemm\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeFiles/27_ampere_3xtf32_fast_accurate_tensorop_gemm.dir/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 23_ampere_gemm_operand_reduction_fusion\u001b[0m\n",
            "[ 44%] Built target 23_ampere_gemm_operand_reduction_fusion\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeFiles/28_ampere_3xtf32_fast_accurate_tensorop_fprop.dir/ampere_3xtf32_fast_accurate_tensorop_fprop.cu.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_fprop_mainloop_fusion\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 22_quaternion_conv\u001b[0m\n",
            "[ 44%] Built target 25_ampere_fprop_mainloop_fusion\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeFiles/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.dir/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu.o\u001b[0m\n",
            "[ 44%] Built target 22_quaternion_conv\n",
            "[ 44%] \u001b[32mBuilding CUDA object examples/30_wgrad_split_k/CMakeFiles/30_wgrad_split_k.dir/30_wgrad_split_k.cu.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CUDA executable 26_ampere_wgrad_mainloop_fusion\u001b[0m\n",
            "[ 45%] Built target 26_ampere_wgrad_mainloop_fusion\n",
            "[ 46%] \u001b[32mBuilding CUDA object examples/31_basic_syrk/CMakeFiles/31_basic_syrk.dir/basic_syrk.cu.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_3d_fprop_mainloop_fusion\u001b[0m\n",
            "[ 46%] Built target 25_ampere_3d_fprop_mainloop_fusion\n",
            "[ 47%] \u001b[32mBuilding CUDA object examples/32_basic_trmm/CMakeFiles/32_basic_trmm.dir/basic_trmm.cu.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CUDA executable 06_splitK_gemm\u001b[0m\n",
            "[ 47%] Built target 06_splitK_gemm\n",
            "[ 48%] \u001b[32mBuilding CUDA object examples/33_ampere_3xtf32_tensorop_symm/CMakeFiles/33_ampere_3xtf32_tensorop_symm.dir/ampere_3xtf32_tensorop_symm.cu.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CUDA executable 27_ampere_3xtf32_fast_accurate_tensorop_gemm\u001b[0m\n",
            "[ 49%] Built target 27_ampere_3xtf32_fast_accurate_tensorop_gemm\n",
            "[ 49%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.21cd46163fea.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.f2656f9851f5.cu.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable 07_volta_tensorop_gemm\u001b[0m\n",
            "[ 50%] Built target 07_volta_tensorop_gemm\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/34_transposed_conv2d/CMakeFiles/34_transposed_conv2d.dir/34_transposed_conv2d.cu.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CUDA executable 31_basic_syrk\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CUDA executable 24_gemm_grouped\u001b[0m\n",
            "[ 51%] Built target 24_gemm_grouped\n",
            "[ 51%] Built target 31_basic_syrk\n",
            "[ 52%] \u001b[32mBuilding CUDA object examples/35_gemm_softmax/CMakeFiles/35_gemm_softmax.dir/gemm_softmax.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CUDA object examples/36_gather_scatter_fusion/CMakeFiles/36_gather_scatter_fusion.dir/gather_scatter_fusion.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 28_ampere_3xtf32_fast_accurate_tensorop_fprop\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 30_wgrad_split_k\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 32_basic_trmm\u001b[0m\n",
            "[ 53%] Built target 28_ampere_3xtf32_fast_accurate_tensorop_fprop\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/37_gemm_layernorm_gemm_fusion/CMakeFiles/37_gemm_layernorm_gemm_fusion.dir/gemm_layernorm.cu.o\u001b[0m\n",
            "[ 53%] Built target 30_wgrad_split_k\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/38_syr2k_grouped/CMakeFiles/38_syr2k_grouped.dir/syr2k_grouped.cu.o\u001b[0m\n",
            "[ 53%] Built target 32_basic_trmm\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/39_gemm_permute/CMakeFiles/39_gemm_permute.dir/gemm_permute.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.da510d6f1a23.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed2bb5981d55.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.22f9e56a890e.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(507): warning: variable \"minus\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(508): warning: variable \"mul\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(509): warning: variable \"exponential\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 34_transposed_conv2d\u001b[0m\n",
            "[ 54%] Built target 34_transposed_conv2d\n",
            "[ 54%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_fixed_seqlen.dir/fused_multihead_attention_fixed_seqlen.cu.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable 36_gather_scatter_fusion\u001b[0m\n",
            "[ 55%] Built target 36_gather_scatter_fusion\n",
            "[ 55%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_variable_seqlen.dir/fused_multihead_attention_variable_seqlen.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1083): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable 33_ampere_3xtf32_tensorop_symm\u001b[0m\n",
            "[ 55%] Built target 33_ampere_3xtf32_tensorop_symm\n",
            "[ 56%] \u001b[32mBuilding CUDA object examples/42_ampere_tensorop_group_conv/CMakeFiles/42_ampere_tensorop_group_conv.dir/ampere_tensorop_group_conv.cu.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CUDA executable 35_gemm_softmax\u001b[0m\n",
            "[ 56%] Built target 35_gemm_softmax\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "[ 57%] \u001b[32mBuilding CUDA object examples/43_ell_block_sparse_gemm/CMakeFiles/43_ell_block_sparse_gemm.dir/ell_block_sparse_gemm.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CUDA executable 37_gemm_layernorm_gemm_fusion\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "[ 58%] Built target 37_gemm_layernorm_gemm_fusion\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "[ 58%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.17417f539ca2.cu.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CUDA executable 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\u001b[0m\n",
            "[ 59%] Built target 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\n",
            "[ 60%] \u001b[32mBuilding CUDA object examples/45_dual_gemm/CMakeFiles/45_dual_gemm.dir/dual_gemm.cu.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CUDA executable 39_gemm_permute\u001b[0m\n",
            "[ 61%] Built target 39_gemm_permute\n",
            "[ 62%] \u001b[32mBuilding CUDA object examples/46_depthwise_simt_conv2dfprop/CMakeFiles/46_depthwise_simt_conv2dfprop.dir/depthwise_simt_conv2dfprop.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CUDA executable 38_syr2k_grouped\u001b[0m\n",
            "[ 63%] Built target 38_syr2k_grouped\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.d1a5010bad63.cu.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CUDA executable 43_ell_block_sparse_gemm\u001b[0m\n",
            "[ 64%] Built target 43_ell_block_sparse_gemm\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5a217a522f90.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e764a5c32d2e.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CUDA executable 42_ampere_tensorop_group_conv\u001b[0m\n",
            "[ 65%] Built target 42_ampere_tensorop_group_conv\n",
            "[ 65%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.00a490a48b54.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.67748a1f69fd.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable 45_dual_gemm\u001b[0m\n",
            "[ 66%] Built target 45_dual_gemm\n",
            "[ 66%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.aaae24350764.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable 46_depthwise_simt_conv2dfprop\u001b[0m\n",
            "[ 66%] Built target 46_depthwise_simt_conv2dfprop\n",
            "[ 67%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a7556fb0c363.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_fixed_seqlen\u001b[0m\n",
            "[ 68%] Built target 41_fused_multi_head_attention_fixed_seqlen\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.cdd7679105ea.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5b5abd9da9bb.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0188c1e5012f.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c1136defa56d.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.4efc263febd5.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b1c066c89ad1.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_variable_seqlen\u001b[0m\n",
            "[ 70%] Built target 41_fused_multi_head_attention_variable_seqlen\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.059188bf387f.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dc6fdf4e3631.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed319cdaa486.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7f69d119038b.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ab4c8725d89a.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.db0920d13551.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.53863dce2b34.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5206989612aa.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a40fdb431023.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.999493302618.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.bbd766ae1258.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.3aca44f87dc7.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.fe5af8d45b59.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2d9ae5d87c7a.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8112247fe822.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.447ac2531506.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b7cdd3426b6c.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b83304556123.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c2bae868a773.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.51d0f833ea6d.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dd6b23b6f933.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.eac06d5a86ae.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ec9d28d7c560.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.25dee0d0208a.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.41f602329ae6.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.9763c0db7135.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2fdf36a298c6.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0a8164c28c5f.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a4cfc29c733f.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8a2ca8aaf7aa.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.977a56f2bc38.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e837fe821ee2.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.6f06994db371.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ae96169b4e45.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a6a0ceb84086.cu.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.19976851ec6d.cu.o\u001b[0m\n",
            "[ 87%] Built target cutlass_library_objs\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX static library libcutlass.a\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX shared library libcutlass.so\u001b[0m\n",
            "[ 88%] Built target cutlass_library_static\n",
            "[ 88%] Built target cutlass_lib\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/10_planar_complex/CMakeFiles/10_planar_complex.dir/planar_complex.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/11_planar_complex_array/CMakeFiles/11_planar_complex_array.dir/planar_complex_array.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cutlass_profiler.cu.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/options.cu.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/performance_report.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/enumerated_types.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gpu_timer.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_allocation.cu.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_context.cu.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cublas_helpers.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cudnn_helpers.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/problem_space.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_2k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/trmm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/symm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv2d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv3d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/sparse_gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CUDA executable 10_planar_complex\u001b[0m\n",
            "[ 98%] Built target 10_planar_complex\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CUDA executable 11_planar_complex_array\u001b[0m\n",
            "[ 99%] Built target 11_planar_complex_array\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable cutlass_profiler\u001b[0m\n",
            "[100%] Built target cutlass_profiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torch-int/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33WhSKg5fAuf",
        "outputId": "6f5536f6-bc73-4b4c-94be-976b949cab14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like someone here\n",
        "\n",
        "https://github.com/NVIDIA/FasterTransformer/issues/381\n",
        "\n",
        "is running into similar issues. After adding:\n",
        "\n",
        "\n",
        "```\n",
        "            include_dirs=['torch_int/kernels/include','submodules/cutlass/include'],\n",
        "```\n",
        "\n",
        "\n",
        "I am now missing cutlass/util/input something something. Luckily, using\n",
        "\n",
        "https://nvidia.github.io/cutlass/files.html\n",
        "\n",
        "I'm able to gain a foggy picture of what the installation was maybe(?) supposed to look like, and I found index_sequence.h inside submodules/cutlass/tools/util/include/cutlass/util/. Turns out that the setup include_dirs needs explicit paths, so I just have to go through and patch up these holes—what I'm confused about is why I have to do this myself?\n",
        "\n",
        "HOLY CROW IT WORKED!\n",
        "```\n",
        "include_dirs=[\n",
        "    'torch_int/kernels/include',\n",
        "    'submodules/cutlass/include',\n",
        "    'submodules/cutlass/tools/util/include',\n",
        "    'submodules/cutlass/tools'\n",
        "]\n",
        "```\n",
        "This is what did it. Bless up\n"
      ],
      "metadata": {
        "id": "pVh9eM_ogT_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gonna try to use the pip as claude recommended\n",
        "\n",
        "#! pip install -e .\n",
        "! python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfpQCEdrKGf8",
        "outputId": "d6db9e18-77b6-4b9a-8bed-00ebb276cac8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating torch_int.egg-info\n",
            "writing torch_int.egg-info/PKG-INFO\n",
            "writing dependency_links to torch_int.egg-info/dependency_links.txt\n",
            "writing top-level names to torch_int.egg-info/top_level.txt\n",
            "writing manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "reading manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torch_int.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/torch_int\n",
            "copying torch_int/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int\n",
            "creating build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/quantization.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "copying torch_int/functional/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/functional\n",
            "creating build/lib.linux-x86_64-cpython-310/torch_int/utils\n",
            "copying torch_int/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/fused.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/linear.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "copying torch_int/nn/bmm.py -> build/lib.linux-x86_64-cpython-310/torch_int/nn\n",
            "creating build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/models/__init__.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "copying torch_int/models/opt.py -> build/lib.linux-x86_64-cpython-310/torch_int/models\n",
            "running build_ext\n",
            "building 'torch_int._CUDA' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/torch_int\n",
            "creating build/temp.linux-x86_64-cpython-310/torch_int/kernels\n",
            "/usr/bin/gcc-9 -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -Itorch_int/kernels/include -Isubmodules/cutlass/include -Isubmodules/cutlass/tools/util/include -Isubmodules/cutlass/tools -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bindings.cpp -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bindings.o -std=c++14 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda-11.3/bin/nvcc -Itorch_int/kernels/include -Isubmodules/cutlass/include -Isubmodules/cutlass/tools/util/include -Isubmodules/cutlass/tools -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/bmm.cu -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DCUDA_ARCH=800 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -ccbin /usr/bin/gcc-9\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda-11.3/bin/nvcc -Itorch_int/kernels/include -Isubmodules/cutlass/include -Isubmodules/cutlass/tools/util/include -Isubmodules/cutlass/tools -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/fused.cu -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/fused.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DCUDA_ARCH=800 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -ccbin /usr/bin/gcc-9\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda-11.3/bin/nvcc -Itorch_int/kernels/include -Isubmodules/cutlass/include -Isubmodules/cutlass/tools/util/include -Isubmodules/cutlass/tools -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda-11.3/include -I/usr/include/python3.10 -c torch_int/kernels/linear.cu -o build/temp.linux-x86_64-cpython-310/torch_int/kernels/linear.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DCUDA_ARCH=800 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_CUDA -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -ccbin /usr/bin/gcc-9\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/bin/g++-9 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/torch_int/kernels/bindings.o build/temp.linux-x86_64-cpython-310/torch_int/kernels/bmm.o build/temp.linux-x86_64-cpython-310/torch_int/kernels/fused.o build/temp.linux-x86_64-cpython-310/torch_int/kernels/linear.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda-11.3/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-cpython-310/torch_int/_CUDA.cpython-310-x86_64-linux-gnu.so -lcublas_static -lcublasLt_static -lculibos -lcudart -lcudart_static -lrt -lpthread -ldl -L/usr/lib/x86_64-linux-gnu/\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/torch_int\n",
            "creating build/bdist.linux-x86_64/egg/torch_int/functional\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/functional/fused.py -> build/bdist.linux-x86_64/egg/torch_int/functional\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/functional/__init__.py -> build/bdist.linux-x86_64/egg/torch_int/functional\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/functional/quantization.py -> build/bdist.linux-x86_64/egg/torch_int/functional\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/functional/bmm.py -> build/bdist.linux-x86_64/egg/torch_int/functional\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/__init__.py -> build/bdist.linux-x86_64/egg/torch_int\n",
            "creating build/bdist.linux-x86_64/egg/torch_int/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/utils/__init__.py -> build/bdist.linux-x86_64/egg/torch_int/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/_CUDA.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/torch_int\n",
            "creating build/bdist.linux-x86_64/egg/torch_int/nn\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/nn/fused.py -> build/bdist.linux-x86_64/egg/torch_int/nn\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/nn/linear.py -> build/bdist.linux-x86_64/egg/torch_int/nn\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/nn/__init__.py -> build/bdist.linux-x86_64/egg/torch_int/nn\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/nn/bmm.py -> build/bdist.linux-x86_64/egg/torch_int/nn\n",
            "creating build/bdist.linux-x86_64/egg/torch_int/models\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/models/__init__.py -> build/bdist.linux-x86_64/egg/torch_int/models\n",
            "copying build/lib.linux-x86_64-cpython-310/torch_int/models/opt.py -> build/bdist.linux-x86_64/egg/torch_int/models\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/functional/fused.py to fused.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/functional/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/functional/quantization.py to quantization.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/functional/bmm.py to bmm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/nn/fused.py to fused.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/nn/linear.py to linear.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/nn/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/nn/bmm.py to bmm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/models/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/models/opt.py to opt.cpython-310.pyc\n",
            "creating stub loader for torch_int/_CUDA.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_int/_CUDA.py to _CUDA.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_int.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_int.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_int.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_int.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "torch_int.__pycache__._CUDA.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/torch_int-0.0.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing torch_int-0.0.0-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/torch_int-0.0.0-py3.10-linux-x86_64.egg\n",
            "Extracting torch_int-0.0.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding torch-int 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/torch_int-0.0.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for torch-int==0.0.0\n",
            "Finished processing dependencies for torch-int==0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up inference"
      ],
      "metadata": {
        "id": "Vsk2IUda1f3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying transformers and huggingface-hub version"
      ],
      "metadata": {
        "id": "9VfvCbMq1qBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# there was no path for some reason\n",
        "import sys\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brBwvh8Qm4y_",
        "outputId": "9bb3a39f-0aec-4182-b4fe-a7f3f4aa41ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/smoothquant')\n",
        "sys.path.append('/content/torch-int')\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flw9Z4eryC4n",
        "outputId": "86d275b7-dade-4931-938c-360438e12072"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/smoothquant', '/content/torch-int']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the issue I'm facing is there not being recognition of smoothquant.opt."
      ],
      "metadata": {
        "id": "_YOs4muPm9Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://stackoverflow.com/questions/487971/is-there-a-standard-way-to-list-names-of-python-modules-in-a-package\n",
        "\n",
        "import imp\n",
        "MODULE_EXTENSIONS = ('.py', '.pyc', '.pyo')\n",
        "\n",
        "def package_contents(package_name):\n",
        "    file, pathname, description = imp.find_module(package_name)\n",
        "    if file:\n",
        "        raise ImportError('Not a package: %r', package_name)\n",
        "    # Use a set because some may be both source and compiled.\n",
        "    return set([os.path.splitext(module)[0]\n",
        "        for module in os.listdir(pathname)\n",
        "        if module.endswith(MODULE_EXTENSIONS)])"
      ],
      "metadata": {
        "id": "jgXJR1AzpTLR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "package_contents('smoothquant')\n",
        "\n",
        "# it's here, so why isn't it being recognized...?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp5tkmLspdRY",
        "outputId": "fb898fa0-1d4c-472e-a77b-86838f294bba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__init__', 'calibration', 'fake_quant', 'opt', 'ppl_eval', 'smooth'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so now the smoothquant.smoothquant.opt somehow recognizes, but import torch_int doesn't work. Now, after 30 minutes, i realize that maybe I need to say torch-int. But sometimes it's torch_int, sometimes it's torch-int, so what am I supposed to do with that? torch-int is the outer, torch_int is the inner.\n",
        "\n",
        "Okay, now it worked. Not sure what changed, maybe it was using pip install instead of a manual run."
      ],
      "metadata": {
        "id": "a7WCF4AGwZUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/huggingface/transformers/issues/27701\n",
        "\n",
        "Now I'm getting AttributeError: type object 'OPTDecoder' has no attribute '_prepare_decoder_attention_mask'\n",
        "with the smoothquant import. This is because of a refactor that happened in November 2023, so I'm going to change opt.py's _prepare_decoder_attention_mask to _prepare_4d_causal_attention_mask. Line 375 in opt.py.\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py\n",
        "\n",
        "Above is also helpful for seeing the documentation of OPTDecoder and why I'm having this issue. I literally don't see _prepare_decoder_attention_mask or _prepare_4d_causal_attention_mask in the OPTDecoder class either, so I'm going to try and revert to the 4.26.0 release.\n",
        "\n",
        "Okay. Welp. That didn't work.\n"
      ],
      "metadata": {
        "id": "-FiAGqFhzmI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip cache purge\n",
        "!pip install transformers==4.34.0 --no-cache-dir\n",
        "\n",
        "#! pip install --force-reinstall transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2VNnI_2N3h",
        "outputId": "a7be1b67-20f4-4209-ea13-c5c2d17a27c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.36.0\n",
            "Uninstalling transformers-4.36.0:\n",
            "  Successfully uninstalled transformers-4.36.0\n",
            "Files removed: 18\n",
            "Collecting transformers==4.34.0\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (0.21.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2.32.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (2024.7.4)\n",
            "Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m213.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m276.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m319.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.21.2\n",
            "    Uninstalling huggingface-hub-0.21.2:\n",
            "      Successfully uninstalled huggingface-hub-0.21.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.20.0 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, it kind of looks like that when I have transformers 4.43.3 (latest), there's no definition for this _prepare attribute, but when I had 4.26.0 there was one. Let me check that I'm not tripping out.\n",
        "\n",
        "Okay, based on this https://github.com/huggingface/transformers/issues/27701 GitHub error, I've decided to use transformers v4.34.0, which seemed to work, because it gave me a different error:\n",
        "\n",
        "```\n",
        "RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\n",
        "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)\n",
        "```\n",
        "\n",
        "Which seems to coincide with a previous error up above when I uninstalled transformers 4.36 and replaced it with 4.34:\n",
        "\n",
        "```\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "datasets 2.20.0 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
        "Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0\n",
        "```\n",
        "\n",
        "So I'm going to now upgrade huggingface-hub to 0.21.2."
      ],
      "metadata": {
        "id": "FhrIxKks7QvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub==0.21.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnE031GshYx8",
        "outputId": "734e7ab2-894e-47fa-88b6-4eb3c2b97d06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-hub==0.21.2\n",
            "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (2024.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.21.2) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.21.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.21.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.21.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.21.2) (2024.7.4)\n",
            "Downloading huggingface_hub-0.21.2-py3-none-any.whl (346 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/346.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.21.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IT WORKED. YES!!!!! THE MODEL IS FINALLY INSTALLING!!!!! Victory from the basement of IIT. I'm committing to git rn.\n",
        "\n",
        "Also, this edit seemed to also fix the odd ```smoothquant.smoothquant.opt``` issue."
      ],
      "metadata": {
        "id": "98qskJPHhrj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for some reason, smoothquant.smoothquant.opt works?\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# yet, when i print the version, it seems like transformers should be able to reference it.\n",
        "# ??\n",
        "\n",
        "import torch_int\n",
        "from smoothquant.opt import Int8OPTForCausalLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYIiWlbgfsq",
        "outputId": "a465ecab-fcb4-4e56-a91a-8e6dfbb7a4b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Evaluator + Quantized Model"
      ],
      "metadata": {
        "id": "Zo_AuHRS7nWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to download anew\n",
        "#model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")\n",
        "\n",
        "# if you have smoothquant downloaded\n",
        "model = Int8OPTForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"/content/drive/MyDrive/opt-30b-smoothquant\",\n",
        "    local_files_only=True,\n",
        "    config=\"/content/drive/MyDrive/opt-30b-smoothquant/config.json\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SeAnMBQA8ESO",
        "outputId": "2c9c72f2-1287-4ee6-be25-99eae842b1a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-adb7002f1f29>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you have smoothquant downloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = Int8OPTForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/opt-30b-smoothquant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3085\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg/smoothquant/opt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInt8OPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;31m# the lm_head weight is automatically tied to the embed tokens weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg/smoothquant/opt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOPTConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInt8OPTDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;31m# Initialize weights and apply final processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg/smoothquant/opt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         self.embed_tokens = nn.Embedding(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embed_proj_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_padding_idx_with_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')"
      ],
      "metadata": {
        "id": "wE5JQSKB6_aj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRJShGahTETC",
        "outputId": "77d83e32-9591-4ce5-fa4d-e85cca6bab84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I'm using the smoothquant_opt_real_int8_demo.ipynb as a guide."
      ],
      "metadata": {
        "id": "raPGwNUwqqMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZtdDcZEk6zOW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from transformers.models.opt.modeling_opt import OPTForCausalLM\n",
        "import gc\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "r5x-01auil_s"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from SmoothQuant examples: https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_opt_real_int8_demo.ipynb\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # tokenize the dataset\n",
        "        def tokenize_function(examples):\n",
        "            example = self.tokenizer(examples['text'])\n",
        "            return example\n",
        "\n",
        "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
        "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, model):\n",
        "        model.eval()\n",
        "        # The task is to predict the last word of the input.\n",
        "        total, hit = 0, 0\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "        latency = 0\n",
        "        for batch in self.dataset:\n",
        "            input_ids = batch['input_ids'].cuda().unsqueeze(0)\n",
        "            label = input_ids[:, -1]\n",
        "            pad_len = 512 - input_ids.shape[1]\n",
        "            input_ids = pad(input_ids, (0, pad_len), value=1)\n",
        "            torch.cuda.synchronize()\n",
        "            start.record()\n",
        "            outputs = model(input_ids)\n",
        "            end.record()\n",
        "            torch.cuda.synchronize()\n",
        "            latency += start.elapsed_time(end)\n",
        "            last_token_logits = outputs.logits[:, -2-pad_len, :]\n",
        "            pred = last_token_logits.argmax(dim=-1)\n",
        "            total += label.size(0)\n",
        "            hit += (pred == label).sum().item()\n",
        "\n",
        "        acc = hit / total\n",
        "        lantecy = latency / len(self.dataset)\n",
        "        return acc, lantecy\n",
        "\n",
        "\n",
        "def print_model_size(model):\n",
        "    # https://discuss.pytorch.org/t/finding-model-size/130275\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    print('Model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "iSgH6q-ZyTzo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-30b')\n",
        "dataset = load_dataset('lambada', split='validation[:50]')\n",
        "evaluator = Evaluator(dataset, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "ed93915d0d334917b8f13d87526640ac",
            "8aad70ea63484aa5a92a5b7a0437ad19",
            "e9ccfda2413a43ec99a6f535e178e7c7",
            "adca1e27830c45b8854102958edcb613",
            "efb9438c98b54307a9eff9daa0d6aa91",
            "1fe2e0a106ea44d7b4a1c027240df674",
            "56f96a5b44604571818fcffd40d6483e",
            "61df33397a6f49fab3f8d9a739f19db1",
            "084b40fb59d146578d473c641880196e",
            "6d5602eb360a40559616369702519f42",
            "15242e99713c4599b6f13955b7f40117"
          ]
        },
        "id": "gVwbkQspzioH",
        "outputId": "8ff8159e-a157-4efd-92b4-f26aff2b4ec1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed93915d0d334917b8f13d87526640ac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_size(model)\n",
        "acc_smoothquant, lantecy_smoothquant = evaluator.evaluate(model)\n",
        "print(\n",
        "    f'SmoothQuant INT8 accuracy: {acc_smoothquant}, per-sample lantecy: {lantecy_smoothquant:.3f}ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zru0smA36tjO",
        "outputId": "cf789c35-0933-4352-d8bc-0ddae0baf991"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 29664.908MB\n",
            "SmoothQuant INT8 accuracy: 0.82, per-sample lantecy: 184.583ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I notice that loading the models definitely takes a lot of system memory usage. Also, unrelated, I had to add the offload_folder argument.\n",
        "\n",
        "See here:\n",
        "https://huggingface.co/blog/accelerate-large-models"
      ],
      "metadata": {
        "id": "mpVaG0WU42Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Non-Quantized Model"
      ],
      "metadata": {
        "id": "i2LbgkQY7fIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp16 = OPTForCausalLM.from_pretrained(\n",
        "    'facebook/opt-30b', torch_dtype=torch.float16, device_map='auto', offload_folder=\"offload\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439,
          "referenced_widgets": [
            "88ad8cafc3514d07953581defbd07b99",
            "c79c4c200af54fd78ce937da23167392",
            "9ecac45caf224265ae5b383ac58bbf58",
            "2233f5c653da406bbd1bde8beb5728de",
            "e6b2d8145ea34f3f821bd210be833122",
            "fb853c5ab2a04b489585e71ac66dbeee",
            "f6894f5d3d5949659f2a34074cf88872",
            "d24729f0709f4d4fb53201bf6fa1d9d2",
            "77e58e2050154c478f36f2ba32683925",
            "bc3865c278c443e6ab6b7f7af4bf4a4e",
            "e12cc7df8d39474d9cfc2a3007ab69a2",
            "b2c2b7ce0e0a48bdad089de7049832c7",
            "bd286a7920ec42ef8d1c0f603106580f",
            "0cb5b32856694749b0a28fa9843be65a",
            "f0c46529142d4e88a28590d113624e7a",
            "c78d34d6d2714e1ea35a5838004efcc6",
            "d2ba6b593de2492eb0e0d4f1c38907f8",
            "938c74e24e6e4d47913651161a36f150",
            "fcc4b2fd0c6e4beeb0a168c4ca83e932",
            "31d99e89d8654d54ae374ed38a687c54",
            "b48d3ce182e940788f961d91aa924441",
            "19b64b8d1cc84a9d95f850f8066b735a",
            "6cdda65f05434341ad2a6e1f9c89754b",
            "a82f25cb7f9e4455a40fc69144e2ec84",
            "c76203f685d34ec3a89bd9e90f99d3ba",
            "4f94d4ecd8664d6084444f32db038603",
            "08da95c3d9b44343981165e8c9084dcb",
            "337773732548452aa0ae01e8dd42a74d",
            "3387729198114b5794b4d616a64635b8",
            "774a15feae0c436c8fe70dea23f4c1d4",
            "df07154c9abd4de3a72c0b385de6796d",
            "9edd189507a545e0869d876db1a58344",
            "05968a0fdc644385aec700818e1408e7",
            "914fbe8e42864bffb27279a5642befe5",
            "6a3fde74d11b4d3986251e69fea3460e",
            "2feb72f5febb48fb893abfa9628d7c7c",
            "7a866ccf114c439a849bcb03977d86bd",
            "0e1ebca02ec44e018f8ca63eecb56aa6",
            "cb3c29cc451e4dcbb08335feab197ad4",
            "c904896b10d64e7388424589e7476cd6",
            "d8c790e6b8044e8e924c6608956a7779",
            "49ea14c4fcca468bb4a6491c2a19f69a",
            "17cff365b85246a190e2a906d0df04ea",
            "7a60f653c43f4392915d7dd757391dac"
          ]
        },
        "id": "fnVv5ZpQz7Mj",
        "outputId": "0fdbeff8-861e-495b-b0a0-16f2ff4cf4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/62.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88ad8cafc3514d07953581defbd07b99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2c2b7ce0e0a48bdad089de7049832c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/9.79G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cdda65f05434341ad2a6e1f9c89754b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "914fbe8e42864bffb27279a5642befe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e71ee135a3d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_fp16 = OPTForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     'facebook/opt-30b', torch_dtype=torch.float16, device_map='auto', offload_folder=\"offload\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;31m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3001\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1490\u001b[0m                     \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1493\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_size(model_fp16)\n",
        "acc_fp16, lantecy_fp16 = evaluator.evaluate(model_fp16)\n",
        "print(f'FP16 accuracy: {acc_fp16}, per-sample lantecy: {lantecy_fp16:.3f}ms')"
      ],
      "metadata": {
        "id": "Wxhtspz-8IEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model_fp16\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "X5mysJGz0Kvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text completion task testing"
      ],
      "metadata": {
        "id": "mnHb9wbdC77y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"A Titan RTX has 24GB of VRAM. I am a big fan of machine learning, and so I need a lot of computing power, like what I will get from the Titan RTX! I think that\"\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "input_ids = tokenizer(sequence)\n",
        "input_ids_tensor = torch.tensor(input_ids['input_ids']).unsqueeze(0).to('cuda') # Convert to tensor, add batch dimension, and move to device\n",
        "\n",
        "# oh so this is how you get an attention mask\n",
        "attention_mask = torch.ones_like(input_ids_tensor)\n",
        "\n",
        "input_ids_tensor.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeyYYHrRB1g1",
        "outputId": "f8623d2a-169c-495d-d56b-27a4467de8fc"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.prepare_inputs_for_generation(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-38Bh-9Bfie",
        "outputId": "0ccfbc9d-8aa7-4aab-a48e-b0c5fd79205d"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': {'input_ids': [2, 250, 23308, 40810, 34, 706, 4377, 9, 8311, 2620, 4, 38, 524, 10, 380, 2378, 9, 3563, 2239, 6, 8, 98, 38, 240, 10, 319, 9, 11730, 476, 6, 101, 99, 38, 40, 120, 31, 5, 23308, 40810, 328, 38, 206, 14], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
              " 'past_key_values': None,\n",
              " 'use_cache': None,\n",
              " 'attention_mask': None}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = attention_mask.expand(input_ids_tensor.shape)\n",
        "\n",
        "generation = model.generate(input_ids=input_ids_tensor, attention_mask=attention_mask, max_length=500)\n",
        "tokenizer.decode(generation[0].tolist()) # Convert generated IDs back to a list before decoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zalOSQU3AbO0",
        "outputId": "73c7049e-051f-4105-e0e6-ca1ed685dcbc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The provided attention mask has length 59, but its length should be 64 (sum of the lengths of current and past inputs)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-0329de3cfdaa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert generated IDs back to a list before decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1607\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2455\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m    945\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smoothquant-0.0.0-py3.10.egg/smoothquant/opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         output = self.old_forward(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmask_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    647\u001b[0m                 \u001b[0;34mf\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;34mf\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The provided attention mask has length 59, but its length should be 64 (sum of the lengths of current and past inputs)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "briyqoJO8sp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}